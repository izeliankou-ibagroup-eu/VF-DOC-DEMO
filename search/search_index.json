{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Visual Flow Knowledge Base!","text":"<p>Visual Flow is a low-code ETL/ELT solution with an open source license that has Apache Spark, Kubernetes, and Argo Workflows under the hood of a drag-and-drop interface.</p>"},{"location":"#our-web-site","title":"Our web site:","text":"<p>Visit our shiny web-site: https://visual-flow.com</p>"},{"location":"#contact-us","title":"Contact us!","text":"<p>Feel free to contacts us: <code>info@visual-flow.com</code></p>"},{"location":"devops/about-visual-flow/","title":"About Visual Flow","text":"<p>Visual Flow is an ETL/ELT tool designed for effective data management via convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li>Can integrate data from heterogeneous sources:</li> <li>Azure Blob Storage</li> <li>AWS S3</li> <li>Cassandra</li> <li>Click House</li> <li>DB2</li> <li>Databricks JDBC (global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>Dataframe (for reading)</li> <li>Google Cloud Storage</li> <li>Elastic Search</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>MS SQL</li> <li>Mongo</li> <li>MySQL/Maria</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>REST API</li> <li>It supports the following file formats:</li> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> <li>Leverage direct connectivity to enterprise applications as sources and targets</li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allows to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverages Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul> <p>Visual Flow application is divided into the following repositories:</p> <ul> <li>Visual-Flow-frontend</li> <li>Visual-Flow-backend</li> <li>Visual-Flow-jobs</li> <li>Visual-Flow-deploy (current)</li> <li>Visual-Flow-backend-db-service</li> <li>Visual-Flow-backend-history-service</li> </ul>"},{"location":"devops/about-visual-flow/#visual-flow-deploy","title":"Visual Flow deploy","text":"<p>Find the latest available version based on your environment: - AWS - Azure - Google - Minikube</p>"},{"location":"devops/about-visual-flow/#contribution","title":"Contribution","text":"<p>Check the official guide.</p>"},{"location":"devops/about-visual-flow/#license","title":"License","text":"<p>Visual Flow is an open-source software licensed under the Apache-2.0 license.</p>"},{"location":"devops/slack-notification/","title":"Connect Visual Flow notifications to a Slack Workspace","text":"<p>In order to allow Visual Flow to send Slack notifications from a pipeline, Visual Flow needs to be added as a bot user to your Slack Workspace.</p> <p>Below are the steps for adding a bot user to your Slack Workspace.</p> <ol> <li> <p>Open the following link.</p> </li> <li> <p>In the <code>'Create an app'</code> window select <code>'From scratch'</code>.</p> </li> <li> <p>In the <code>'Name app &amp; choose workspace'</code> window, populate the <code>'App name'</code> field (for example with 'Visual Flow') and in the <code>'Pick a workspace to develop your app in'</code> field select your workspace. Then click the <code>'Create App'</code> button.</p> </li> <li> <p>Choose the <code>OAuth &amp; Permissions</code> tab on the left sidebar.</p> </li> <li> <p>Below the <code>Bot Token Scopes</code> click on <code>'Add an OAuth Scope'</code> and select the following scopes:</p> <pre><code>users:read (View people in a workspace)\nusers:read.email (View email addresses of people in a workspace)\nchat:write (Send messages as Visual flow)\nchat:write.public (Send messages to channels Visual flow isn't a member of)\nchannels:read (View basic information about public channels in a workspace)\ngroups:read (View basic information about private channels that Visual flow has been added to)\nim:read (View basic information about direct messages that Visual flow has been added to)\nmpim:read (View basic information about group direct messages that Visual flow has been added to)\n</code></pre> </li> <li> <p>Above in the <code>'OAuth Tokens for Your Workspace'</code> section click on <code>'Install to Workspace'</code> then click the <code>'Allow'</code> button.</p> </li> <li> <p>Copy the generated <code>'Bot User OAuth Token'</code> and paste it to the <code>SLACK_API_TOKEN</code> variable in values.yaml in the downloaded repository.</p> </li> <li> <p>Return to the installation guide from which you were redirected to this doc and continue installing.</p> </li> </ol> <p>More information about 'Add a bot user' can be found here.</p>"},{"location":"devops/amazon/","title":"Installation Visual Flow to Amazon Elastic Kubernetes Service (EKS)","text":""},{"location":"devops/amazon/#prerequisites","title":"Prerequisites","text":"<p>To install Visual Flow you should have the following software already installed:</p> <ul> <li>AWS CLI (install)</li> <li>kubectl (install)</li> <li>eksctl (install)</li> <li>Helm CLI (install)</li> <li>Git (install)</li> </ul> <p>IMPORTANT: all the actions are recommended to be performed from the admin/root AWS account.</p> <p>If you have just installed the AWS CLI, then you need to log in using following command:</p> <p><code>aws configure</code></p>"},{"location":"devops/amazon/#create-an-eks-cluster","title":"Create an EKS cluster","text":"<p>Visual Flow should be installed on an EKS cluster. For the full functionality - recomended use regular AWS EKS claster (if you need Backend &amp; UI). In case if you need just backend and limited functions of frontend UI (without db &amp; history services) - you can use and Fargate claster.</p> <p>You can create cluster using following commands:</p>"},{"location":"devops/amazon/#eks-fargate","title":"EKS Fargate:","text":"<pre><code>export CLUSTER_NAME=visual-flow\n\neksctl create cluster \\\n--fargate \\\n--name $CLUSTER_NAME \\\n--region us-east-1 \\\n--with-oidc \\\n--full-ecr-access \\\n--external-dns-access \\\n--alb-ingress-access\n\n# duration: ~20min\n# if creation failed delete cluster using following command and repeat from beginning\n# eksctl delete cluster --region us-east-1 --name $CLUSTER_NAME\n\n# check access\nkubectl get nodes\nkubectl get pods --all-namespaces\n</code></pre>"},{"location":"devops/amazon/#eks-regular-cluster-ec2-instance-type-m5large-with-one-node","title":"EKS Regular cluster (EC2 instance type 'm5.large' with one Node)","text":"<p>IMPORTANT: You should have at least 1 large node (8GB memory and 2 vCPU) to be able to run VF application on your cluster. <pre><code>export CLUSTER_NAME=visual-flow\n\neksctl create cluster \\\n--name $CLUSTER_NAME \\\n--region us-east-1 \\\n--with-oidc \\\n--ssh-access \\\n--full-ecr-access \\\n--external-dns-access \\\n--alb-ingress-access \\\n--instance-types=m5.large \\\n--managed \\\n--nodes 1\n\n# duration: ~30min\n# if creation failed delete cluster using following command and repeat from beginning\n# eksctl delete cluster --region us-east-1 --name $CLUSTER_NAME\n\n# check access\nkubectl get nodes\nkubectl get pods --all-namespaces\n</code></pre></p> <p>For additional info check following guide:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html</p>"},{"location":"devops/amazon/#connect-to-existing-eks-cluster-from-local-machine","title":"Connect to existing EKS cluster from local machine","text":"<p>If you have an EKS cluster, you can connect to it using the following command:</p> <p><code>aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt;</code></p>"},{"location":"devops/amazon/#check-access-to-eks-cluster","title":"Check access to EKS cluster","text":"<p>Run the following command to check access to the EKS cluster from the local machine:</p> <p><code>kubectl get nodes</code></p> <p>If you get the message \"<code>error: You must be logged in to the server (Unauthorized)</code>\", you can try to fix it using the following guide:</p> <p>https://aws.amazon.com/premiumsupport/knowledge-center/eks-api-server-unauthorized-error/</p> <p>If you have access to the EKS cluster on a different computer, you can on another computer try to provide access to the cluster for the local machine using the following guide:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html</p> <p>For more information on how to use EKS on fargate check the following guide:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/fargate-getting-started.html</p>"},{"location":"devops/amazon/#install-an-aws-load-balancer-alb-to-eks","title":"Install an AWS Load Balancer (ALB) to EKS","text":"<p>AWS Load Balancer allows you to access applications on EKS from the Internet by hostname. If you don't have it installed, then install it. You can install ALB using following commands:</p> <pre><code># add ALB policy\ncurl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.8.1/docs/install/iam_policy.json\n# the following command will fail if the policy already exists\naws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json\n\n# create SA for ALB\nexport ACCOUNT_ID=&lt;ACCOUNT_ID&gt;\neksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller --attach-policy-arn=arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve\n\n# install ALB via helm chart\nhelm repo add eks https://aws.github.io/eks-charts || helm repo update\n\n# set correct VPC ID for ALB\nVPC_ID_TMP=$(aws eks describe-cluster --name $CLUSTER_NAME | grep vpc- | cut -d ':' -f 2)\nVPC_ID=$(echo $VPC_ID_TMP)\n\nhelm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=$CLUSTER_NAME --set region=us-east-1 --set vpcId=$VPC_ID --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller -n kube-system\n\n# wait until all pods will be ready\nkubectl get pods --all-namespaces\n</code></pre> <p>For additional info check following guide:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html</p>"},{"location":"devops/amazon/#install-an-efs-controller-to-eks-for-automatic-pvc-provisioning","title":"Install an EFS Controller to EKS (for automatic PVC provisioning)","text":"<p>How to install Amazon EFS CSI driver:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html</p> <p>GitHub source of EFS CSI controller:</p> <p>https://github.com/kubernetes-sigs/aws-efs-csi-driver</p> <p>Depend from your choice - you can use or Dynamic provisioning (PV &amp; PVC will be created and mounted to StorageClass automatically):</p> <p>https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/dynamic_provisioning/README.md</p> <p>...or Static provisioning (you will need to create PV by yourself with required config):</p> <p>https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/README.md</p>"},{"location":"devops/amazon/#install-redis-postgresql","title":"Install Redis &amp; PostgreSQL","text":"<p>Some functionality of VF app requires to have Redis &amp; PosgreSQL dbs. Both of them with custom and default configs included in installation as a separate helm charts (values files with source from bitnami repo). </p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/tree/amazon/charts/dbs</p> <p>You can get them and install on you cluster using following commands:</p> <p>Add 'bitnami' repository to helm repo list <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre> 1. Redis (for Session and Job's execution history)</p> <p><code>helm install redis -f bitnami-redis/values.yaml bitnami/redis</code></p> <ol> <li>PostgreSQL (History service)</li> </ol> <p><code>helm install pgserver -f bitnami-postgresql/values.yaml bitnami/postgresql</code></p> <p>FYI: Just in case better to save output of these command (it contains helpful info with short guide, how to get access to pod &amp; dbs and show default credentials).</p>"},{"location":"devops/amazon/#install-visual-flow","title":"Install Visual Flow","text":"<ol> <li> <p>Clone (or download) the Amazon branch from Visual-Flow-deploy repository on your local computer using following command:</p> <p><code>git clone -b amazon https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-deploy</code></p> </li> <li> <p>Go to the directory \"visual-flow\" of the downloaded \"Visual-Flow-Deploy\" repository with the following command:</p> <p><code>cd Visual-Flow-deploy/charts/visual-flow</code></p> </li> <li> <p>(Optional) Configure Slack notifications in values.yaml using following guide:</p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/main/SLACK_NOTIFICATION.md</p> </li> <li> <p>Set superusers in values.yaml.</p> <p>New Visual Flow users will have no access in the app. The superusers(admins) need to be configured to manage user access. Specify the superusers real GitHub nicknames in values.yaml in the yaml list format:</p> <pre><code>superusers:\n  - your-github-nickname\n  # - another-superuser-nickname\n</code></pre> </li> <li> <p>If you have installed kube-metrics then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the kube-metrics installed using the following command:</p> <pre><code>kubectl top pods\n</code></pre> <p>Output if the kube-metrics isn't installed:</p> <p><code>error: Metrics API not available</code></p> <p>If the kube-metrics isn't installed then go to step 6.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <pre><code>...\nkube-metrics:\n  install: false\n</code></pre> </li> </ol> </li> <li> <p>If you have installed Argo workflows then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the Argo workflows installed using the following command:</p> <pre><code>kubectl get workflow\n</code></pre> <p>Output if the Argo workflows isn't installed:</p> <p><code>error: the server doesn't have a resource type \"workflow\"</code></p> <p>If the Argo workflows isn't installed then go to step 7.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <pre><code>...\nargo:\n  install: false\nvf-app:\n  backend:\n    configFile:\n      argoServerUrl: &lt;Argo-Server-URL&gt;\n</code></pre> </li> </ol> </li> <li> <p>Install the app using the updated values.yaml file with the following command:</p> <p><code>helm upgrade -i vf-app . -f values.yaml</code></p> </li> <li> <p>Check that the app is successfully installed and all pods are running with the following command:</p> <p><code>kubectl get pods --all-namespaces</code></p> </li> <li> <p>Get the generated app's hostname with the following command:</p> <p><code>kubectl get svc vf-app-frontend -o yaml | grep hostname | cut -c 17-</code></p> <p>Replace the string <code>&lt;HOSTNAME_FROM_SERVICE&gt;</code> with the generated hostname in the next steps.</p> </li> <li> <p>Create a GitHub OAuth app:</p> <ol> <li>Go to GitHub user's OAuth apps (<code>https://github.com/settings/developers</code>) or organization's OAuth apps (<code>https://github.com/organizations/&lt;ORG_NAME&gt;/settings/applications</code>).</li> <li>Click the Register a new application or the New OAuth App button.</li> <li>Fill the required fields:<ul> <li>Set Homepage URL to <code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/</code></li> <li>Set Authorization callback URL to <code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/callback</code></li> </ul> </li> <li>Click the Register application button.</li> <li>Replace \"DUMMY_ID\" with the Client ID value in values.yaml.</li> <li>Click Generate a new client secret and replace in values.yaml \"DUMMY_SECRET\" with the generated Client secret value (Please note that you will not be able to see the full secret value later).</li> </ol> </li> <li> <p>Update 'uiHost' (<code>uiHost: https://&lt;HOSTNAME_FROM_SERVICE&gt;</code>) and 'STRATEGY_CALLBACK_URL' (<code>STRATEGY_CALLBACK_URL: https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/callback</code>) values in values.yaml. </p> </li> <li> <p>Upgrade the app in EKS cluster using updated values.yaml:</p> <p><code>helm upgrade vf-app . -f values.yaml</code></p> </li> <li> <p>Wait until the update is installed and all pods are running:</p> <p><code>kubectl get pods --all-namespaces</code></p> </li> </ol>"},{"location":"devops/amazon/#use-visual-flow","title":"Use Visual Flow","text":"<ol> <li> <p>All Visual Flow users (including superusers) need active Github account in order to be authenticated in application. Setup Github profile as per following steps:</p> <ol> <li>Navigate to the account settings</li> <li>Go to Emails tab: set email as public by unchecking Keep my email addresses private checkbox</li> <li>Go to Profile tab: fill in Name and Public email fields</li> </ol> </li> <li> <p>Open the app's web page using the following link:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/</code></p> </li> <li> <p>See the guide on how to work with the Visual Flow at the following link: Visual_Flow_User_Guide.pdf</p> </li> <li> <p>For each project Visual Flow generates a new namespace. For each namespace, you should create a Fargate profile to allow running jobs and pipelines in the corresponding project.</p> </li> </ol> <p>First, create the project in the app, open it and check the URL of the page. It will have the following format:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/&lt;NAMESPACE&gt;/overview</code></p> <p>Get namespace from this URL and use in the following command to create fargate profile:</p> <pre><code>`eksctl create fargateprofile --cluster &lt;CLUSTER_NAME&gt; --region &lt;REGION&gt; --name vf-app --namespace &lt;NAMESPACE&gt;`\n</code></pre>"},{"location":"devops/amazon/#delete-visual-flow","title":"Delete Visual Flow","text":"<ol> <li> <p>If the app is no longer required, you can delete it using the following command:</p> <p><code>helm uninstall vf-app</code></p> </li> <li> <p>Check that everything was successfully deleted with the command:</p> <p><code>kubectl get pods --all-namespaces</code></p> </li> </ol>"},{"location":"devops/amazon/#delete-additional-components","title":"Delete additional components","text":"<p>If you do no need them anymore - you can also delete and these additional components:</p> <ol> <li>Redis &amp; PostgreSQL databases</li> </ol> <p><code>helm uninstall redis</code></p> <p><code>helm uninstall pgserver</code></p> <ol> <li>EFS &amp; LoadBalancer Controllers</li> </ol> <p><code>helm uninstall aws-efs-csi-driver</code></p> <p><code>helm uninstall aws-load-balancer-controller</code></p>"},{"location":"devops/amazon/#delete-eks","title":"Delete EKS","text":"<ol> <li>If the EKS is no longer required, you can delete it using the following guide:</li> </ol> <p><code>eksctl delete cluster --name visual-flow</code></p> <p>https://docs.aws.amazon.com/eks/latest/userguide/delete-cluster.html</p>"},{"location":"devops/azure/","title":"Installation Visual Flow to Azure Kubernetes Service (AKS)","text":""},{"location":"devops/azure/#prerequisites","title":"Prerequisites","text":"<p>IMPORTANT: This installation requires access to our private Container Registry. Please contact us to get access: info@visual-flow.com</p> <p>To install Visual Flow on AKS you should have the following software on your local\\master machine already installed:</p> <ul> <li>Azure CLI (install)</li> <li>kubectl (install)</li> <li>Helm CLI (install)</li> <li>Git (install)</li> </ul> <p>IMPORTANT: all the actions are recommended to be performed from the Microsoft account with owner privileges.</p> <p>If you have just installed the Azure CLI, then you need to log in using following command:</p> <p><code>az login</code></p> <p>And create a Resource Group (if you do not have any) with the location that suits you best:</p> <p><code>az group create --name MyResourceGroup --location centralus</code></p>"},{"location":"devops/azure/#create-aks-cluster","title":"Create AKS cluster","text":"<p>IMPORTANT: if you are new to AKS, please read about AKS cluster: cluster types,config params and pricing (https://learn.microsoft.com/en-us/azure/aks/)</p> <p>Visual Flow should be installed on AKS cluster. You can create AKS cluster using following commands:</p> <pre><code>export RESOURCE_GROUP=MyResourceGroup\nexport CLUSTER_NAME=visual-flow\nexport LOCATION=centralus\nexport NUM_NODES=2\nexport NUM_ZONES=1\naz aks create --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME --node-count $NUM_NODES --location $LOCATION --zones $NUM_ZONES --generate-ssh-keys\n\n# check access\naz aks get-credentials --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME\nkubectl get pods --all-namespaces\n</code></pre> <p>For additional info check following guide:</p> <p>https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-cli</p>"},{"location":"devops/azure/#connect-to-existing-aks-cluster-from-local-machine","title":"Connect to existing AKS cluster from local machine","text":"<p>If you have AKS cluster, you can connect to it using the following command:</p> <p><code>az aks get-credentials --resource-group &lt;YOUR_RESOURCE_GROUP&gt; --name &lt;YOUR_CLUSTER_NAME&gt;</code></p>"},{"location":"devops/azure/#install-visual-flow","title":"Install Visual Flow","text":"<ol> <li> <p>Clone (or download) the azure branch from Visual-Flow-deploy repository on your local computer using following command:</p> <p><code>git clone -b azure https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-azure-deploy</code></p> </li> <li> <p>Go to the directory \"visual-flow\" of the downloaded \"Visual-Flow-Deploy\" repository with the following command:</p> <p><code>cd Visual-Flow-azure-deploy/charts/visual-flow</code></p> </li> <li> <p>(Optional) Configure Slack notifications (replace <code>YOUR_SLACK_TOKEN</code>) in values-az.yaml using the following guide:</p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/main/SLACK_NOTIFICATION.md</p> </li> <li> <p>Set superusers in values-az.yaml.</p> <p>New Visual Flow users will have no access in the app. The superusers(admins) need to be configured to manage user access. Specify the superusers real GitHub nicknames in values-az.yaml in the yaml list format:</p> <pre><code>superusers:\n  - your-github-nickname\n  # - another-superuser-nickname\n</code></pre> </li> <li> <p>(Optional) If you want, you can install kube-metrics then update values-az.yaml file according to the example below. </p> <ol> <li> <p>Check if the kube-metrics installed using the following command:</p> <pre><code>kubectl top pods\n</code></pre> <p>Output if the kube-metrics isn't installed:</p> <p><code>error: Metrics API not available</code></p> <p>If the kube-metrics is already installed then go to step 6.</p> </li> <li> <p>Edit values-az.yaml file according to the example below:</p> <pre><code>...\nkube-metrics:\n  install: true\n</code></pre> </li> </ol> </li> <li> <p>If you have installed Argo workflows then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the Argo workflows installed using the following command:</p> <pre><code>kubectl get workflow\n</code></pre> <p>Output if the Argo workflows isn't installed:</p> <p><code>error: the server doesn't have a resource type \"workflow\"</code></p> <p>If the Argo workflows isn't installed then go to step 7.</p> </li> <li> <p>Edit values-az.yaml file according to the example below:</p> <pre><code>...\nargo:\n  install: false\nvf-app:\n  backend:\n    configFile:\n      argoServerUrl: &lt;Argo-Server-URL&gt;\n</code></pre> </li> </ol> </li> <li> <p>Install Redis database. If you have installed Redis database then just update values.yaml file according to the example below.</p> <ol> <li> <p>Create a namespace for Redis. In current configuration =redis: <pre><code>kubectl create namespace &lt;REDIS_NAMESPACE&gt;\n</code></pre> <li> <p>Update Helm repo:</p> <pre><code>helm repo update\n</code></pre> </li> <li> <p>Install Redis with predefined values.yaml (./charts/dbs/bitnami-redis/values.yaml):</p> <pre><code>helm install -n &lt;REDIS_NAMESPACE&gt; redis bitnami/redis -f ../dbs/bitnami-redis/values.yaml\n</code></pre> </li> <li> <p>Update <code>redis.host</code> and <code>redis.password</code> of backend and frontend section in values-az.yaml file according to the example below:</p> <pre><code>...\nredis:\n  host: redis-master.&lt;REDIS_NAMESPACE&gt;.svc.cluster.local \n  port: 6379\n# username: ${REDIS_USER}\n  password: # &lt;REDIS_PASSWORD&gt;\n  database: 1\n...\n  frontend:\n    deployment:\n      variables:\n        ...\n        REDIS_HOST: redis-master.&lt;REDIS_NAMESPACE&gt;.svc.cluster.local\n        ...\n      secretVariables:\n        ...\n        REDIS_PASSWORD: # &lt;REDIS_PASSWORD&gt;\n</code></pre> </li> <li> <p>Install PostgreSQL database. If you have installed PostgreSQL database then just update values.yaml file according to the example below.</p> <ol> <li> <p>Create a namespace for PostgreSQL. In current configuration =postgres: <pre><code>kubectl create namespace &lt;PostgreSQL_NAMESPACE&gt;\n</code></pre> <li> <p>Update Helm repo:</p> <pre><code>helm repo update\n</code></pre> </li> <li> <p>Install PostgreSQL with predefined values.yaml (./charts/dbs/bitnami-postgresql/values.yaml):</p> <pre><code>helm install -n &lt;PostgreSQL_NAMESPACE&gt; postgresql bitnami/postgresql -f ../dbs/bitnami-postgresql/values.yaml\n</code></pre> </li> <li> <p>Update <code>PG_URL</code>, <code>PG_USER</code> and <code>PG_PASS</code> in values-az.yaml file according to the example below:</p> <p><pre><code>...\nhistoryserv:\n  configFile:\n    postgresql:\n      PG_URL: jdbc:postgresql://postgresql.&lt;PostgreSQL_NAMESPACE&gt;.svc.cluster.local:5432/postgres # postgres_url\n      PG_USER: # postgres_username\n      PG_PASS: # postgres_password\n</code></pre> 9. Prepare namespace for Visual Flow.</p> </li> <li> <p>Create a namespace for Visual Flow. In current configuration =visual-flow: <pre><code>kubectl create namespace &lt;VF_NAMESPACE&gt;\n</code></pre> <li> <p>(Optional) Set visual-flow namespace to be default in your profile:</p> <pre><code>kubectl config set-context --current --namespace=&lt;VF_NAMESPACE&gt;\n</code></pre> </li> <li> <p>Install the app using the updated values-az.yaml file with the following command:</p> <p><code>helm install visual-flow . -f values-az.yaml -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Check that the app is successfully installed and all pods are running with the following command:</p> <p><code>kubectl get pods -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Get the generated app's hostname with the following command:</p> <p><code>kubectl get svc visual-flow-frontend -n &lt;VF_NAMESPACE&gt; -o yaml | grep -i clusterIP: | cut -c 14-</code></p> <p>Replace the string <code>&lt;EXTERNAL_IP_FROM_SERVICE&gt;</code> with the generated hostname in the next steps.</p> </li> <li> <p>Create a GitHub OAuth app:</p> <ol> <li>Go to GitHub user's OAuth apps (<code>https://github.com/settings/developers</code>) or organization's OAuth apps (<code>https://github.com/organizations/&lt;ORG_NAME&gt;/settings/applications</code>).</li> <li>Click the Register a new application or the New OAuth App button.</li> <li>Fill the required fields:<ul> <li>Set Homepage URL to <code>https://&lt;EXTERNAL_IP_FROM_SERVICE&gt;/vf/ui/</code></li> <li>Set Authorization callback URL to <code>https://&lt;EXTERNAL_IP_FROM_SERVICE&gt;/vf/ui/callback</code></li> </ul> </li> <li>Click the Register application button.</li> <li>Replace \"DUMMY_ID\" with the Client ID value in values-az.yaml.</li> <li>Click Generate a new client secret and replace in values-az.yaml \"DUMMY_SECRET\" with the generated Client secret value (Please note that you will not be able to see the full secret value later).</li> </ol> </li> <li> <p>Update STRATEGY_CALLBACK_URL value in values-az.yaml to <code>https://&lt;EXTERNAL_IP_FROM_SERVICE&gt;/vf/ui/callback</code></p> </li> <li> <p>Upgrade the app in EKS cluster using updated values.yaml:</p> <p><code>helm upgrade visual-flow . -f values-az.yaml -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Wait until the update is installed and all pods are running:</p> <p><code>kubectl get pods -n &lt;VF_NAMESPACE&gt;</code></p> </li>"},{"location":"devops/azure/#use-visual-flow","title":"Use Visual Flow","text":"<ol> <li> <p>All Visual Flow users (including superusers) need active Github account in order to be authenticated in application. Setup Github profile as per following steps:</p> <ol> <li>Navigate to the account settings</li> <li>Go to Emails tab: set email as public by unchecking Keep my email addresses private checkbox</li> <li>Go to Profile tab: fill in Name and Public email fields</li> </ol> </li> <li> <p>Open the app's web page using the following link:</p> <p><code>https://&lt;EXTERNAL_IP_FROM_SERVICE&gt;/vf/ui/</code></p> </li> <li> <p>See the guide on how to work with the Visual Flow at the following link: Visual_Flow_User_Guide.pdf</p> </li> <li> <p>For each project Visual Flow (VF) generates a new namespace. </p> </li> </ol> <p>IMPORTANT: For each namespace there is a PVC that will be created and assigned automatically (<code>vf-pvc</code>) in RWX mode (<code>read\\write-many</code>). AKS has default storage clases to provision PVs in RWX mode such as Azure files, but it uses CIFS protocol that does not allow to change file permissions of the files in that PV (https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/could-not-change-permissions-azure-files). This option is required for Visual Flow volumes (temporary files), so Azure files could not be used for stable work. We recommend to install third-party storage class in RWX mode, such as rook-ceph to be able to work with files in RWX mode. You can read about how to install rook-ceph on AKS cluster here: rook-ceph-on-aks-guide-azure.md</p>"},{"location":"devops/azure/#stop-start-aks-cluster","title":"Stop  Start AKS cluster","text":"<ol> <li> <p>If you want to stop temporary your AKS cluster and VF application, you can simply stop the cluster:</p> <p><code>az aks stop --name &lt;YOUR_CLUSTER_NAME&gt; --resource-group &lt;YOUR_RESOURCE_GROUP_NAME&gt;</code></p> </li> <li> <p>Once you need it back:</p> <p><code>az aks start --name &lt;YOUR_CLUSTER_NAME&gt; --resource-group &lt;YOUR_RESOURCE_GROUP_NAME&gt;</code></p> </li> </ol>"},{"location":"devops/azure/#delete-visual-flow","title":"Delete Visual Flow","text":"<ol> <li> <p>If the app is no longer required, you can delete it using the following command:</p> <p><code>helm uninstall vf-app -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Check that everything was successfully deleted with the command:</p> <p><code>kubectl get pods -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Delete Visual Flow namespace:      <code>kubectl delete namespace &lt;VF_NAMESPACE&gt;</code></p> </li> </ol>"},{"location":"devops/azure/#delete-aks","title":"Delete AKS","text":"<ol> <li> <p>If the AKS is no longer required, you can delete it using the following guide:</p> <p>https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-delete</p> </li> </ol>"},{"location":"devops/azure/rook-ceph-on-aks-guide-azure/","title":"Rook Ceph on AKS","text":"<p>If you want to understand how to install rook-ceph on AKS and configure it yourself, you can read about it here: https://devpress.csdn.net/k8s/62ebdd3d89d9027116a0fa0d.html</p> <p>If you want to install it just for Visual Flow projects you can follow the guide below:</p> <p>For this install we use: - Rook-ceph v1.12.0 https://github.com/rook/rook/tree/v1.12.0 - We expect that you already have AKS installed and configured including Visual Flow install index.md - One extra nodepool for AKS to be created manually for rook-ceph (in this guide we call it <code>rookcephfs</code>). - 1 node (Standard_B2ms machine). - We recommend to use at least 2vCPU and 8Gb RAM for this rook-ceph node to be able to start and run it succesfully. If you try with worse configuration this install may fail with unexpected errors\\warnings.</p> <ol> <li>Add Storage Node to your AKS cluster:</li> </ol> <pre><code>az aks nodepool add --cluster-name &lt;YOUR_CLUSTER_NAME&gt; \\\n--name rookecephfs --resource-group &lt;YOUR_RESOURCE_GROUP&gt; \\ \n--node-count 1 \\\n--node-taints storage-node=true:NoSchedule\n</code></pre> <ol> <li>Make sure you can see your new node:</li> </ol> <pre><code>kubectl get nodes\n</code></pre> <ol> <li>Install commons.yaml:</li> </ol> <pre><code>kubectl create -f rook-ceph_1.12.0/common.yaml\n</code></pre> <ol> <li>Create AKS operator operator-aks.yaml:</li> </ol> <pre><code>kubectl create -f rook-ceph_1.12.0/operator-aks.yaml\n</code></pre> <ol> <li>Create AKS CephCluster cluster-aks.yaml. Defaults are: 1 node, 10G storage and rookcephfs nodepool name. If you need to change storage, please take a look at this file and update <code>spec.mon.volumeClaimTemplate.spec.resources.requests.storage</code> and <code>spec.storageClassDeviceSets.volumeClaimTemplates.spec.resources.requests.storage</code>. If you have another nodepool name, please update <code>spec.storageClassDeviceSets.placement.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchExpressions.values</code>.</li> </ol> <pre><code>kubectl create -f rook-ceph_1.12.0/cluster-aks.yaml\n</code></pre> <ol> <li>Wait a couple of minutes and check status using next commands. If CephCluster status is not HEALTH_OK or you do not see rook-ceph-osd-0-<code>CephID</code> pods then you need to verify previous steps to make sure you did not missed and check logs. Status check and expected output:</li> </ol> <pre><code>$ kubectl get -n rook-ceph CephCluster.ceph.rook.io/rook-ceph\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE     PHASE   MESSAGE                        HEALTH      EXTERNAL   FSID\nrook-ceph   /var/lib/rook     1          2m42s   Ready   Cluster created successfully   HEALTH_OK              d6e47925-b86e-4b12-b10d-aea1d39c327e\n\n$ kubectl get pods -n rook-ceph | grep -i rook-ceph-osd\nrook-ceph-osd-0-8844ff8c-slfc5                                    1/1     Running     0          60s\nrook-ceph-osd-prepare-set1-data-0mvx2c-6s6wb                      0/1     Completed   0          82s\n</code></pre> <ol> <li>Once you have HEALTH_OK status and OSD pod running you can create a cephFilesystem and a rook-ceph storageclass:</li> </ol> <pre><code>kubectl create -f rook-ceph_1.12.0/storageclass-fs-aks.yaml\n</code></pre> <ol> <li> <p>Now you need to change your default storage class to be <code>rook-cephfs</code> that we just created. How to change your default storage class: https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/</p> </li> <li> <p>Done! Once you changed your default storageclass all your new Visual Flow project PVCs will be provisioned by rook-ceph. Create a new project using Visual Flow and verify that you have BOUND status of your PVC <code>vf-pvc</code> in Visual Flow project namespace:</p> </li> </ol> <pre><code>$ get pvc -n &lt;YOUR_VF_PROJECT_NAMESPACE&gt;\nNAME     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE\nvf-pvc   Bound    pvc-65ff9574-513e-4ee2-8053-2476423811f2   2Gi        RWX            rook-cephfs-storage   1d\n</code></pre>"},{"location":"devops/databricks/","title":"Installation Guide","text":""},{"location":"devops/databricks/#step-1","title":"Step 1","text":"<p>Log in to your Databricks account.</p> <p></p>"},{"location":"devops/databricks/#step-2","title":"Step 2","text":"<p>Ensure your Databricks workspace is configured and running as per your requirements. Get Databricks workspace URL.</p> <p>Click your workspace name \u2192 Manage account</p> <p></p> <p>Click your email address \u2192 Workspaces \u2192 select URL</p> <p></p>"},{"location":"devops/databricks/#step-3","title":"Step 3","text":"<p>Select Authentication type: OAuth or Personal Access Token</p>"},{"location":"devops/databricks/#for-oauth-option","title":"For [OAuth] option:","text":"<p>Generate OAuth secret</p> <p>Click workspace name \u2192 Manage account</p> <p></p> <p>User management \u2192 Service principals \u2192 Click your service principal or create a new one</p> <p></p> <p>Click Generate secret</p> <p></p>"},{"location":"devops/databricks/#for-personal-access-token-option","title":"For [Personal Access Token] option:","text":"<p>Generate a Personal access token if you haven\u2019t done already.</p> <p>Click your user name \u2192 Settings</p> <p></p> <p>Developer \u2192 Generate new token button</p> <p></p>"},{"location":"devops/databricks/#step-4","title":"Step 4","text":"<p>Ensure you have a volume (whatever volume path you like) where Visual Flow will put prerequisite files. For example:</p> <p>Click path in your volume \u2192 copy button</p> <p></p>"},{"location":"devops/databricks/#step-5","title":"Step 5","text":"<p>Click the \u201c+\u201d button to create a new project. In the Create Project window specify: Project Name, Description, Databricks workspace URL, Authentication type (OAuth or Personal access token), Volume path, and click the \u201cSave\u201d button.</p> <p>\u201c+\u201d select</p> <p></p> <p>[OAuth]</p> <p></p> <p>[Personal Access Token]</p> <p></p>"},{"location":"devops/google/","title":"Installation Visual Flow to Google Kubernetes Engine (GKE)","text":"<ol> <li>Prerequisite Installation<ul> <li>Setting up prerequisite tools</li> <li>Clone Visual Flow repository</li> <li>Create GKE cluster</li> <li>Configure GitHub OAuth</li> <li>Install Redis &amp; PostgreSQL</li> </ul> </li> <li>Installation of Visual Flow</li> <li>Use Visual Flow</li> <li>Stop  Start GKE cluster</li> <li>Delete Visual Flow</li> </ol>"},{"location":"devops/google/#prerequisite-installation","title":"Prerequisite Installation","text":"<p>[!NOTE] If you have any concerns, please contact us: info@visual-flow.com</p>"},{"location":"devops/google/#setting-up-prerequisite-tools","title":"Setting up prerequisite tools","text":"<p>To install Visual Flow on GKE you should have the following software on your local\\master machine already installed:</p> <ul> <li>Google CLI (install)</li> <li>kubectl (install)</li> <li>gke-gcloud-auth-plugin (install)</li> <li>Helm CLI (install)</li> <li>Git (install)</li> </ul> <p>[!IMPORTANT] All the actions are recommended to be performed from the Google account with \"Project: Owner\" privileges.</p> <p>If you have just installed the Google CLI, then you need to log in using the following command:</p> <pre><code>gcloud auth login\n</code></pre>"},{"location":"devops/google/#clone-visual-flow-repository","title":"Clone Visual Flow repository","text":"<p>Clone (or download) the google branch from Visual-Flow-deploy repository on your local computer using the following command:</p> <pre><code>git clone -b google https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-GCP-deploy\ncd Visual-Flow-GCP-deploy\n</code></pre>"},{"location":"devops/google/#create-gke-cluster","title":"Create GKE cluster","text":"<p>[!IMPORTANT] If you are new to GKE, please read about Google cloud cluster: cluster types, config params and pricing (https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters)</p> <p>Visual Flow should be installed on GKE cluster. We recommend to use Standard cluster, because Autopilot cluster has some extra limitations and worse application performance. You can create GKE cluster using the following commands:</p> <pre><code>export CLUSTER_NAME=visual-flow\nexport ZONE_NAME=us-central1-b\nexport NUM_NODES=2\ngcloud container clusters create $CLUSTER_NAME --region $ZONE_NAME --num-nodes=$NUM_NODES\n\n# check access\nkubectl get nodes\nkubectl get pods --all-namespaces\n</code></pre> <p>[!TIP] For additional info check the following guide:</p> <p>https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster</p>"},{"location":"devops/google/#connect-to-existing-gke-cluster-from-the-local-machine","title":"Connect to existing GKE cluster from the local machine","text":"<p>If you already have GKE cluster, you can connect to it using the following command:</p> <pre><code>gcloud container clusters get-credentials &lt;CLUSTER_NAME&gt; --zone &lt;ZONE_NAME&gt; --project &lt;GOOGLE_PROJECT_NAME&gt;\n</code></pre>"},{"location":"devops/google/#configure-github-oauth","title":"Configure GitHub OAuth","text":"<ol> <li>Go to GitHub user's OAuth apps (<code>https://github.com/settings/developers</code>) or organization's OAuth apps (<code>https://github.com/organizations/&lt;ORG_NAME&gt;/settings/applications</code>)</li> <li>Click the Register a new application or the New Org OAuth App button</li> <li>Fill in the required fields:<ul> <li>Set Homepage URL to <code>https://visual-flow-dummy-url.com/vf/ui/</code></li> <li>Set Authorization callback URL to <code>https://visual-flow-dummy-url.com/vf/ui/callback</code></li> </ul> </li> <li>Click the Register application button</li> <li>Click Generate a new client secret</li> <li>Replace \"DUMMY_ID\" and \"DUMMY_SECRET\" with your <code>Client ID\\Client secret</code> pair value in values.yaml.</li> </ol> <p>[!NOTE] Make sure to copy client secret before you refresh or close the web page. The value will be hidden. In case you lost your client secret, just create a new <code>Client ID\\Client secret</code> pair.</p> <p><code>visual-flow-dummy-url.com</code> is a dummy URL. After Install do not forget to update Homepage URL and Authorization callback URL fields.</p>"},{"location":"devops/google/#install-redis-postgresql","title":"Install Redis &amp; PostgreSQL","text":"<p>Some functionality of VF app requires to have Redis &amp; PosgreSQL dbs. Both of them with custom and default configs included in installation as a separate helm charts (values files with source from bitnami repo). </p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/tree/amazon/charts/dbs</p> <p>You can get them and install on you cluster using the following commands:</p> <p>Add 'bitnami' repository to helm repo list <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre> 1. Redis (for Session and Job's execution history)</p> <p><code>helm install redis -f charts/dbs/bitnami-redis/values.yaml bitnami/redis</code></p> <ol> <li>PostgreSQL (History service)</li> </ol> <p><code>helm install pgserver -f charts/dbs/bitnami-postgresql/values.yaml bitnami/postgresql</code></p> <p>FYI: Just in case, it is better to save output of these commands (it contains helpful info with short guide, how to get access to pod &amp; dbs and show default credentials).</p>"},{"location":"devops/google/#install-visual-flow","title":"Install Visual Flow","text":"<p>[!NOTE] Current installation is configured to work on <code>default</code> namespace of your cluster. But your Visual Flow projects are stored in <code>vf-&lt;projectname&gt;</code> namespaces.</p> <ol> <li> <p>Go to the directory \"visual-flow\" of the downloaded \"Visual-Flow-Deploy\" repository with the following command:</p> <pre><code>cd charts/visual-flow\n</code></pre> </li> <li> <p>(Optional) Configure Slack notifications (replace <code>YOUR_SLACK_TOKEN</code>) in values.yaml using the following guide:</p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/main/SLACK_NOTIFICATION.md</p> </li> <li> <p>Set superusers in values.yaml.</p> <p>New Visual Flow users will have no access in the app. The superusers(admins) need to be configured to manage user access. Specify the superusers real GitHub nicknames in values.yaml in the yaml list format:</p> <pre><code>superusers:\n  - your-github-nickname\n  # - another-superuser-nickname\n</code></pre> </li> <li> <p>If you have installed kube-metrics then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the kube-metrics installed using the following command:</p> <pre><code>kubectl top pods\n</code></pre> <p>Output if the kube-metrics isn't installed:</p> <p><code>error: Metrics API not available</code></p> <p>If the kube-metrics isn't installed then go to step 6.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <pre><code>...\nkube-metrics:\n  install: false\n</code></pre> </li> </ol> </li> <li> <p>If you have installed Argo workflows then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the Argo workflows installed using the following command:</p> <pre><code>kubectl get workflow\n</code></pre> <p>Output if the Argo workflows isn't installed:</p> <p><code>error: the server doesn't have a resource type \"workflow\"</code></p> <p>If the Argo workflows isn't installed then go to step 7.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <pre><code>...\nargo:\n  install: false\nvf-app:\n  backend:\n    configFile:\n      argoServerUrl: &lt;Argo-Server-URL&gt;\n</code></pre> </li> </ol> </li> <li> <p>Install the app using the updated values.yaml file with the following command:</p> <pre><code>helm upgrade -i vf-app . -f values.yaml -n default\n</code></pre> </li> <li> <p>Check that the app is successfully installed and all pods are running with the following command:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> </li> <li> <p>Get the generated app's hostname\\IP with the following command:</p> <pre><code>kubectl get svc visual-flow-frontend -n default -o yaml | grep -i clusterIP: | cut -c 14-\n</code></pre> <p>Replace the string <code>&lt;HOSTNAME_FROM_SERVICE&gt;</code> with the generated hostname\\IP in the next steps. Replace <code>visual-flow-dummy-url.com</code> from OAuth step with your hostname or IP in Homepage URL and Authorization callback URL fields. Save changes.</p> </li> <li> <p>Update 'host' (<code>host: https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/</code>) and 'STRATEGY_CALLBACK_URL' (<code>STRATEGY_CALLBACK_URL: https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/callback</code>) values in values.yaml. </p> </li> <li> <p>Upgrade the app in EKS cluster using updated values.yaml:</p> <pre><code>helm upgrade vf-app . -f values.yaml -n default\n</code></pre> </li> <li> <p>Wait until the update is installed and all pods are running:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> </li> </ol>"},{"location":"devops/google/#use-visual-flow","title":"Use Visual Flow","text":"<ol> <li> <p>All Visual Flow users (including superusers) need active Github account in order to be authenticated in application. Setup Github profile as described in the next steps:</p> <ol> <li>Navigate to the account settings</li> <li>Go to Emails tab: set email as public by unchecking Keep my email addresses private checkbox</li> <li>Go to Profile tab: fill in Name and Public email fields</li> </ol> </li> <li> <p>Open the app's web page using the following link:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/</code></p> </li> <li> <p>See the guide on how to work with the Visual Flow at the following link: Visual_Flow_User_Guide.pdf</p> </li> <li> <p>For each project Visual Flow (VF) generates a new namespace. </p> </li> </ol> <p>[!IMPORTANT] For each namespace there is a PVC that will be created and assigned automatically (<code>vf-pvc</code>) in RWX mode (<code>read\\write-many</code>). GKE has default storage classes to provision PV in RWX modes (f.e. <code>standard-rwx</code>) that uses Cloud Filestore API service, but it has limitation size (1Tb-10Tb) when VF usually use small disks (f.e. 2G per project\\namespace). So, each VF project will cost you at least 200$/month. There is an open ticket for this feature: (https://issuetracker.google.com/issues/193108375?pli=1). Instead, you can use NFS and assign it to each new namespace, but it have to be assigned manually for each VF project. You can read here about how to create yourself NFS server on your Google cloud (https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/google/GCP_NFS_server_how_to.md). If you do not need to know how to create a new NFS server and assign VF to it you can skip this section.</p> <p>First, create the project in the app, open it and check the URL of the page. It will have the following format:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/&lt;NAMESPACE&gt;/overview</code></p> <p>Now delete automatically created PVC in that : <pre><code>kubectl delete pvc vf-pvc -n &lt;NAMESPACE&gt;\n</code></pre> <p>Once you have your NFS server ready, create a file with next content and replace items from comments in the header.</p> <pre><code>```yaml\n# &lt;PV_NAME&gt; - name of Persistent Volume for your project. # vf-pvc-testing\n# &lt;STORAGE_SIZE&gt; - storage size that you want to assign to this VF project. # 2G\n# &lt;NFS_HOST&gt; - NFS server ip. # YOUR_NFS_IP\n# &lt;NFS_PATH&gt; - PATH to shared folder in your NFS you want to use in this VF project. # /share\n# &lt;NAMESPACE&gt; - VF project namespace for jobs. # vf-test\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: &lt;PV_NAME&gt;\nspec:\n  storageClassName: \"\"\n  persistentVolumeReclaimPolicy: Delete\n  capacity:\n    storage: &lt;STORAGE_SIZE&gt;\n  accessModes:\n    - ReadWriteMany\n  nfs:\n    server: &lt;NFS_HOST&gt;\n    path: &lt;NFS_PATH&gt;\n---   \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: vf-pvc\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  storageClassName: \"\"\n  volumeName: &lt;PV_NAME&gt;\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: &lt;STORAGE_SIZE&gt;\n```\n\nDeploy your file using next command:\n\n```bash\nkubectl apply -f &lt;your_yaml_file_with_pvc&gt; -n &lt;NAMESPACE&gt;\n```\n</code></pre>"},{"location":"devops/google/#stop-start-gke-cluster","title":"Stop  Start GKE cluster","text":"<ol> <li> <p>If you want to stop temporary your GKE cluster and VF application, the easiest way is to scale down the number of nodes:</p> <pre><code>gcloud container clusters resize visual-flow --node-pool default-pool --num-nodes 0 --zone &lt;ZONE_NAME&gt;\n</code></pre> </li> <li> <p>Once you need it back, just restore num-nodes back:</p> <pre><code>gcloud container clusters resize visual-flow --node-pool default-pool --num-nodes &lt;NUM_NODES&gt; --zone &lt;ZONE_NAME&gt;\n</code></pre> </li> </ol>"},{"location":"devops/google/#delete-visual-flow","title":"Delete Visual Flow","text":"<ol> <li> <p>If the app is no longer required, you can delete it using the following command:</p> <pre><code>helm uninstall vf-app -n default\n</code></pre> </li> <li> <p>Check that everything was successfully deleted with the command:</p> <pre><code>kubectl get pods -n default\n</code></pre> </li> </ol>"},{"location":"devops/google/#delete-gke","title":"Delete GKE","text":"<ol> <li> <p>If the GKE is no longer required, you can delete it using the following guide:</p> <p>https://cloud.google.com/sdk/gcloud/reference/container/clusters/delete</p> </li> </ol>"},{"location":"devops/google/gcp-nfs-server-how-to-google/","title":"GCP NFS Server","text":""},{"location":"devops/google/gcp-nfs-server-how-to-google/#steps-to-create-nfs-on-google-cloud-platform-gcp","title":"Steps to create NFS on Google Cloud Platform (GCP):","text":"<ol> <li> <p>Create a Virtual Machine (VM) on GCP      <pre><code>gcloud compute instances create vf-nfs-server \\\n--boot-disk-size=20GB \\\n--image=ubuntu-minimal-2204-jammy-v20230617 \\\n--image-project=ubuntu-os-cloud \\\n--machine-type=f1-micro \\\n--tags=vf-nfs \\\n--zone=us-central1-b\n</code></pre></p> </li> <li> <p>Connect via SSH. press yes for everything if asked about different   zone press n, so it autodetect your vm zone      <pre><code>gcloud compute ssh vf-nfs-server\n</code></pre></p> </li> <li> <p>Install NFS (inside VM)      <pre><code>sudo apt update\nsudo apt install -y nfs-kernel-server\n</code></pre></p> </li> <li> <p>Create a folder for share      <pre><code>sudo mkdir /share\nsudo chown nobody:nogroup /share\nsudo chmod 777 /share\n</code></pre></p> </li> <li> <p>Add this foulder to nfs exports</p> <p>Install vim (or any other tool for text edit) on your VM  <pre><code>sudo apt install -y vim\n</code></pre></p> <p>Edit exports  <pre><code>sudo vim /etc/exports\n</code></pre></p> <p>Add the next line to /etc/exports  <pre><code>/share *(rw,sync,no_subtree_check)\n</code></pre></p> </li> <li> <p>Restart nfs-kernel-server      <pre><code>sudo systemctl restart nfs-kernel-server\n</code></pre></p> <p>Confirm the directory is being shared  <pre><code>sudo exportfs\n</code></pre>  Output should be like <code>/share world</code>.</p> <p>Done, log out from the machine  <pre><code>exit\n</code></pre></p> </li> <li> <p>(Optional) if you want to use NFS outside of your google project. Add firewall rules      <pre><code>gcloud compute firewall-rules create nfs \\\n--allow=tcp:111,udp:111,tcp:2049,udp:2049 --target-tags=nfs\n</code></pre></p> </li> </ol>"},{"location":"devops/minikube/","title":"Installation Visual Flow to Local Minikube","text":""},{"location":"devops/minikube/#prerequisites","title":"Prerequisites","text":"<p>To install Visual Flow you should have the following software installed:</p> <ul> <li>Git (install)</li> <li>kubectl (install)</li> <li>Helm CLI (install)</li> <li>Minikube (install)</li> </ul> <p>By default VF projects require 4 CPU and 6GB of RAM. We do not recommend to reduce these settings (to avoid job run issues), but you may increase these values based on your workload.</p>"},{"location":"devops/minikube/#create-minikube-cluster","title":"Create Minikube cluster","text":"<p>In this example we will use the HyperV VM driver, which is recommended for the Windows OS family. But depending on your system or requirements - you can also use Docker, VirtualBox, Podman, KVM2 etc.</p> <p>Also kubernetes version is 1.25.4, since current latest one (1.27.2) caused problem with GitOAuth Authentification (you may get issue like 'Failed to obtain access token'). So at least on this version with HyperV driver app was tested and works without any problem.</p> <p>You can create simple cluster in Minikube using following commands:</p> <pre><code>minikube start --cpus 4 --memory 6g --disk-size 20g --delete-on-failure=true --driver hyperv --kubernetes-version=v1.25.4 -p visual-flow \n\n# duration: ~5-10min\n\n# if creation failed - delete cluster using following command and repeat from beginning:\n\n#&gt; minikube delete -p visual-flow\n</code></pre> <p>When cluster is ready - you can switch default profile to this cluster, check running pods and cluster IP: <pre><code>minikube profile visual-flow\n\nkubectl get pods -A\n\nminikube ip\n# on this IP will be available VF application and other services\n</code></pre></p> <p>For additional info about Minikube check following guide:  https://minikube.sigs.k8s.io/docs/start</p>"},{"location":"devops/minikube/#install-redis-postgresql","title":"Install Redis &amp; PostgreSQL","text":"<p>Some functionality of VF app requires to have Redis &amp; PosgreSQL dbs. Both of them with custom and default configs included in installation as a separate helm charts (values files with source from bitnami repo). </p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/tree/minikube/charts/dbs</p> <p>You can get them and install on you cluster using following actions.</p> <ul> <li> <p>Add 'bitnami' repository to helm repo list <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre></p> </li> <li> <p>Clone Minikube branch from Visual-Flow-deploy repository and go to Visual-Flow-deploy/charts/dbs <pre><code>git clone -b minikube https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-deploy\n\ncd Visual-Flow-deploy/charts/dbs\n</code></pre></p> </li> <li> <p>Redis (for Session and Job's execution history) <pre><code>helm install redis -f bitnami-redis/values.yaml bitnami/redis\n</code></pre></p> </li> <li> <p>PostgreSQL (History service) <pre><code>helm install pgserver -f bitnami-postgresql/values.yaml bitnami/postgresql\n</code></pre></p> </li> <li> <p>Check that both services Ready and Running <pre><code>&gt; kubectl get pods\nNAME                    READY   STATUS    RESTARTS   AGE\npgserver-postgresql-0   1/1     Running   0          2m59s\nredis-master-0          1/1     Running   0          3m23s\n</code></pre> FYI: Just in case better to save output of these command (it contains helpful info with short guide how to get access to pod &amp; dbs and show default credentials).</p> </li> </ul> <p>Go back to your main folder to proceed with Visual Flow installation <pre><code>cd ../../..\n</code></pre></p>"},{"location":"devops/minikube/#install-visual-flow","title":"Install Visual Flow","text":"<ol> <li> <p>Make sure you already have the Minikube branch from Visual-Flow-deploy repository on your local computer, if no - clone it using the following command: <pre><code>git clone -b minikube https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-deploy\n</code></pre></p> </li> <li> <p>Go to the directory \"visual-flow\" of the downloaded \"Visual-Flow-Deploy\" repository with the following command: <pre><code>cd Visual-Flow-deploy/charts/visual-flow\n</code></pre></p> </li> <li> <p>(Optional). Configure Slack notifications in values.yaml using following guide:</p> </li> </ol> <p>Configure Slack notification</p> <ol> <li> <p>Set superusers in values.yaml.</p> <p>New Visual Flow users will have no access in the app. The superusers(admins) need to be configured to manage user access. Specify the superusers real GitHub nicknames in values.yaml in the yaml list format:</p> <pre><code>superusers:\n  - your-github-nickname\n  # - another-superuser-nickname\n</code></pre> </li> <li> <p>If you have installed kube-metrics then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the kube-metrics installed using the following command:</p> <pre><code>kubectl top pods\n</code></pre> <p>Output if the kube-metrics isn't installed:</p> <p><code>error: Metrics API not available</code></p> <p>If the kube-metrics isn't installed then go to step 6.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <pre><code>...\nkube-metrics:\n  install: false\n</code></pre> </li> </ol> </li> <li> <p>If you have installed Argo workflows then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the Argo workflows installed using the following command:</p> <pre><code>kubectl get workflow\n</code></pre> <p>Output if the Argo workflows isn't installed:</p> <p><code>error: the server doesn't have a resource type \"workflow\"</code></p> <p>If the Argo workflows isn't installed then go to step 7.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <pre><code>...\nargo:\n  install: false\nvf-app:\n  backend:\n    configFile:\n      argoServerUrl: &lt;Argo-Server-URL&gt;\n</code></pre> </li> </ol> </li> <li> <p>Install the app using the updated values.yaml file with the following command:</p> <pre><code>helm upgrade -i vf-app . -f values.yaml\n</code></pre> </li> <li> <p>Check that the app is successfully installed and all pods are running with the following command:</p> <pre><code>kubectl get pods -A\n</code></pre> </li> <li> <p>Get the IP of your cluster with following command:</p> <pre><code>minikube ip\n</code></pre> <p>Replace the string <code>&lt;HOSTNAME_FROM_SERVICE&gt;</code> with the generated hostname in the next steps.</p> </li> <li> <p>Create a GitHub OAuth app:</p> <ol> <li>Go to GitHub user's OAuth apps (<code>https://github.com/settings/developers</code>) or organization's OAuth apps (<code>https://github.com/organizations/&lt;ORG_NAME&gt;/settings/applications</code>).</li> <li>Click the Register a new application or the New OAuth App button.</li> <li>Fill the required fields:<ul> <li>Set Homepage URL to <code>https://&lt;HOSTNAME_FROM_SERVICE&gt;:30910/vf/ui/</code></li> <li>Set Authorization callback URL to <code>https://&lt;HOSTNAME_FROM_SERVICE&gt;:30910/vf/ui/callback</code></li> </ul> </li> <li>Click the Register application button.</li> <li>Replace \"DUMMY_ID\" with the Client ID value in values.yaml.</li> <li>Click Generate a new client secret and replace in values.yaml \"DUMMY_SECRET\" with the generated Client secret value (Please note that you will not be able to see the full secret value later).</li> </ol> </li> <li> <p>Update 'uiHost' (<code>uiHost: https://&lt;HOSTNAME_FROM_SERVICE&gt;</code>) and 'STRATEGY_CALLBACK_URL' (<code>STRATEGY_CALLBACK_URL: https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/callback</code>) values in values.yaml. </p> </li> <li> <p>Upgrade release using updated 'values.yaml':</p> <pre><code>helm upgrade vf-app . -f values.yaml\n</code></pre> </li> <li> <p>Wait until the update is installed and all pods are running:</p> <pre><code>kubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"devops/minikube/#use-visual-flow","title":"Use Visual Flow","text":"<ol> <li> <p>All Visual Flow users (including superusers) need active Github account in order to be authenticated in application. Setup Github profile as per following steps:</p> <ol> <li>Navigate to the account settings</li> <li>Go to Emails tab: set email as public by unchecking Keep my email addresses private checkbox</li> <li>Go to Profile tab: fill in Name and Public email fields</li> </ol> </li> <li> <p>Open the app's web page using the following link:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;:30910/vf/ui/</code></p> </li> <li> <p>See the guide on how to work with the Visual Flow at the following link: Visual_Flow_User_Guide.pdf</p> </li> </ol>"},{"location":"devops/minikube/#delete-visual-flow","title":"Delete Visual Flow","text":"<ol> <li> <p>If the app is no longer need, you can delete it using the following command:</p> <p><code>helm uninstall vf-app</code></p> </li> <li> <p>Check that everything was successfully deleted with the command:</p> <p><code>kubectl get pods --all-namespaces</code></p> </li> </ol>"},{"location":"devops/minikube/#delete-additional-components","title":"Delete additional components","text":"<p>If you do no need them anymore - you can also delete and these additional components:</p> <ul> <li>Redis &amp; PostgreSQL databases</li> </ul> <p><code>helm uninstall redis</code></p> <p><code>helm uninstall pgserver</code></p>"},{"location":"devops/minikube/#delete-minikube-cluster-and-profile","title":"Delete Minikube cluster and profile","text":"<p>If this cluster is no longer need - you can delete it using the following command:</p> <p><code>minikube delete -p visual-flow</code></p>"},{"location":"devops/minikube/#helpful-links-about-minikube","title":"Helpful links about Minikube:","text":"<ul> <li>Minikube Start (https://minikube.sigs.k8s.io/docs/start)</li> <li>Minikube Basic Control (https://minikube.sigs.k8s.io/docs/handbook/controls)</li> <li>Minikube Dashboard (https://minikube.sigs.k8s.io/docs/handbook/dashboard)</li> <li>Minikube Tutorials (https://minikube.sigs.k8s.io/docs/tutorials)</li> <li>Minikube FAQ (https://minikube.sigs.k8s.io/docs/faq)</li> </ul>"},{"location":"stages-description/ReadWrite/","title":"Readwrite","text":""},{"location":"stages-description/ReadWrite/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Choose storage <code>chooseStorage</code> Storage <code>storage</code> Depending on the selected storage type, the corresponding parameters are displayed. Note <code>note</code> Read stage can have only one outgoing arrow. If need more, you can use Cache Stage. Write mode <code>writeMode</code> 1. Append. Append mode means that when saving a DataFrame to a data source, if data/tablealready exists, contents of the DataFrame are expected to be appended to existing data.2. Overwrite. It means that when saving a DataFrame to a data source, if data/table alreadyexists, existing data is expected to be overwritten by the contents of the DataFrame.3. Error if table exists. This mode means that when saving a DataFrame to a data source, if dataalready exists, an exception is expected to be thrown. Truncate mode <code>truncateModeCascade</code> 3. Cascade. The cascade truncation use to overcome fail, if the target table has a primary keywhich is referenced as a foreign key in other tables. The cascade truncation that would not onlydelete the data from target table, but also from other tables that use target table's primary key as a foreign key constraint. Truncate mode <code>truncateMode</code> 1. None. No truncation would occur, but the target table will be deleted and recreated. Note that all the indexes, constraints, etc that were defined for this table will be lost.2. Simple. The standard truncation that would delete the data from the target table in the efficientway, but would leave table's indexes, constraints and other modifiers intact.However note that if the target table has a primary key which is referenced as a foreign key in other tables, the truncation will fail. To overcome this either drop constraints manually(outside of VF) prior to accessing the table with VF. There are no additional fields. <code>noAdditionalFields</code>"},{"location":"stages-description/ReadWrite/#db2","title":"Db2","text":"UI Name Key (Code) Description JDBC URL <code>JDBCURL</code> A database connection URL is a string that your DBMS JDBC driver uses to connect to a database.The connection URL format for the driver is:jdbc:database-type://hostname:port[;property=value[;...]]. User <code>user</code> User name for a JDBC connection. Password <code>password</code> Password for a JDBC connection. Custom SQL <code>customSql</code> Displays the schema and the table fields, if you choose false. If you choose true, you will be able to write your own SQL code in the provided field. Schema <code>schema</code> Schema that the table used for reading belongs to. Table <code>table</code> Database table name. SQL statement <code>optionDbtable</code> The code of your own SQL query.- You have to specify both schema name and table name in your custom SQL query.- Column names must be surrounded by double quotes.- Any syntactic error in SQL query wouldn't be highlighted. Certificate data (optional) <code>certData</code> Enter certification data if needed. Prepare query <code>prepareQuery</code> A prefix that will form the final query together with query. As the specified query will be parenthesized as a subquery in the FROM clause and some databases do not support all clauses in subqueries, the prepareQuery property offers a way to run such complex queries. As an example, spark will issue a query of the following form to the JDBC Source. SELECT  FROM () spark_gen_alias Session init statement <code>sessionInitStatement</code> After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option(\"sessionInitStatement\", \"\"\"BEGIN execute immediate \"alter session set \"_serial_direct_read\"=true\"; END;'\"') Partition column, Lower bound, Upper bound <code>partitionColumn</code> These options must all be specified if any of them is specified. In addition, numPartitions must be specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric, date, or timestamp column from the table in question. Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading. Number of partitions <code>numberOfpartitions</code> The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before writing. Fetch size <code>fetchSize</code> The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows). Batch size <code>batchSize</code> The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. Create table options <code>createTableOptions</code> This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.). Create table column types <code>createTableColumnTypes</code> The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: \"name CHAR(64), comments VARCHAR(1024)\"). The specified types should be valid spark sql data types. Custom schema <code>customSchema</code> The custom schema to use for reading data from JDBC connectors. For example, \"id DECIMAL(38, 0), name STRING\". You can also specify partial fields, and the others use the default type mapping. For example, \"id DECIMAL(38, 0)\". The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults."},{"location":"stages-description/ReadWrite/#azure","title":"Azure","text":"UI Name Key (Code) Description Path in container <code>pathInContainer</code> Path in the container. Container <code>container</code> Name of the basic containers for holding your data. Storage account <code>storageAccount</code> Name of the storage account. Authentication type <code>authenticationType</code> Can be either storage account key or SAS token. Storage account key <code>storageAccountKey</code> Value of the storage account key. SAS token <code>SASToken</code> Value of the SAS token. Partition by <code>partitionBy</code> Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like:- year=2016/month=01/- year=2016/month=02/ Write mode <code>writeMode</code> 1. Append. Append mode means that when saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.2. Overwrite. It means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.3. Error if table exists. This mode means that when saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown."},{"location":"stages-description/ReadWrite/#google","title":"Google","text":"UI Name Key (Code) Description Path to JSON key file <code>pathToKey</code> Path to the key file. Partition by <code>partitionBy</code> Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like:- year=2016/month=01/- year=2016/month=02/ Write mode <code>writeMode</code> 1. Append. Append mode means that when saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.2. Overwrite. It means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.3. Error if table exists. This mode means that when saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown. Path in bucket <code>pathInBucket</code> Path in the bucket. Bucket <code>bucket</code> Name of the basic containers for holding your data."},{"location":"stages-description/ReadWrite/#clickhouse","title":"Clickhouse","text":"UI Name Key (Code) Description Host <code>host</code> ClickHouse hostname or IP address. Port <code>port</code> ClickHouse port. User <code>user</code> Name of the ClickHouse user. Password <code>password</code> Password of the ClickHouse user. Custom SQL <code>customSql</code> Displays the schema and the table fields, if you choose false. If you choose true, you will be able to write your own SQL code in the provided field. Database <code>database</code> ClickHouse database name. Schema <code>schema</code> Schema that the table used for reading belongs to. Table <code>table</code> Database table name. SQL statement <code>optionDbtable</code> The code of your own SQL query.- You have to specify both schema name and table name in your custom SQL query.- Column names must be surrounded by double quotes.- Any syntactic error in SQL query wouldn't be highlighted."},{"location":"stages-description/ReadWrite/#cos","title":"Cos","text":"UI Name Key (Code) Description Authentication type <code>authType</code> Authentication type which displays accessKey and secretKey, if you choose HMAC, or iamApiKey and iamServiceId, if you choose IAM. Endpoint <code>endpoint</code> The endpoint that a service will talk to, for example, 's3.us-south.objectstorage.softlayer.net'. Access key <code>accessKey</code> The COS access key ID. Secret key <code>secretKey</code> The COS secret access key. Api key <code>iamApiKey</code> The COS IAM api key. ServiceId key <code>iamServiceId</code> The COS IAM service id. Bucket <code>bucket</code> Name of the basic containers for holding your data. Path in bucket <code>pathInBucket</code> Path in the bucket. File format <code>format</code> Spark DataFrame format.For CSV, please, specify Header and Delimiter. Partition By (optional) <code>partitionBy</code> Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like:- year=2016/month=01/- year=2016/month=02/ avroSchema <code>avroSchema</code> Avro can be used to define the data schema for a record's value.This schema describes the fields allowed in the value, along with their data types."},{"location":"stages-description/ReadWrite/#aws","title":"Aws","text":"UI Name Key (Code) Description Anonymous access <code>anonymousAccess</code> Anonymous access which does not display accessKey and secretKey. SSL <code>ssl</code> Whether to enable SSL connections on all supported protocols.False by default. Endpoint <code>endpoint</code> The endpoint that a service will talk to, for example, 's3.us-south.objectstorage.softlayer.net'. Access key <code>accessKey</code> The AWS access key ID. Secret key <code>secretKey</code> The AWS secret access key. Bucket <code>bucket</code> Name of the basic containers for holding your data. Path in bucket <code>pathInBucket</code> Path in the bucket. File format <code>format</code> Spark DataFrame format.For CSV, please, specify Header and Delimiter. Partition By (optional) <code>partitionBy</code> Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like:- year=2016/month=01/- year=2016/month=02/ avroSchema <code>avroSchema</code> Avro can be used to define the data schema for a record's value.This schema describes the fields allowed in the value, along with their data types."},{"location":"stages-description/ReadWrite/#elastic","title":"Elastic","text":"UI Name Key (Code) Description Nodes <code>nodes</code> Node is an instance of an Elasticsearch where you can store, index, and search data. Port <code>port</code> Port to bind to for incoming HTTP requests. User <code>user</code> User name for connection. Password <code>password</code> Password for connection. Index <code>index</code> The index where the document resides. SSL <code>ssl</code> Whether to enable SSL connections on all supported protocols.False by default.If it set to True then Certificate data is available, but it is optional. Certificate data (optional) <code>certData</code> Enabled when SSL is set to True.Enter certification data for SSL connection if needed."},{"location":"stages-description/ReadWrite/#mongo","title":"Mongo","text":"UI Name Key (Code) Description Database <code>database</code> MongoDB database name. Collection <code>collection</code> MongoDB collection name. Host <code>host</code> MongoDB hostname or IP address. Port <code>port</code> MongoDB port. User <code>user</code> Name of the MongoDB user. Password <code>password</code> Password of the MongoDB user. SSL <code>ssl</code> Enables/Disables SSL connection. Does not work with self-signed certificates."},{"location":"stages-description/ReadWrite/#cassandra","title":"Cassandra","text":"UI Name Key (Code) Description Keyspace <code>keyspace</code> Name of the keyspace to connect to. Table <code>table</code> Name of the table to connect to. Cluster (optional) <code>cluster</code> Name of the cluster to connect to. Host <code>host</code> Contact point to connect to the Cassandra cluster. Port <code>port</code> Cassandra native connection port. SSL <code>ssl</code> Enable secure connection to Cassandra cluster. Username <code>username</code> Login name for password authentication. Password <code>password</code> Password for password authentication. Push down enabled <code>pushdownEnabled</code> Whether to use pushdown optimizations. Certificate data (optional) <code>certData</code> Custom certificate for SSL connection."},{"location":"stages-description/ReadWrite/#redis","title":"Redis","text":"UI Name Key (Code) Description Host <code>host</code> Host or IP of the initial node we connect to. The connector will read the cluster topology from the initial node, so there is no need to provide the rest of the cluster nodes. Port <code>port</code> The initial node's TCP Redis port. Password <code>password</code> The initial node's AUTH password. SSL <code>ssl</code> Set to true to use TLS. currently it's not allowed to pass client's certificate. Model [binary, hash] <code>model</code> Defines the Redis model used to persist DataFrame, default is hash. Key column <code>keyColumn</code> When writing - specifies unique column used as a Redis key, by default a key is auto-generated.When reading - specifies column name to store hash key. Read mode [key, pattern] <code>readMode</code> Defines the way that the read operation would be handled. If \"key\" is selected, then the read would be done based on the \"table\" field. In case of the \"pattern\", a provided pattern(option \"keysPattern\") will dictate what Redis keys will be read. Table <code>table</code> The table name is used to organize Redis keys in a namespace. Keys pattern <code>keysPattern</code> Spark-Redis tries to extract the key based on the key pattern: if the pattern ends with * and it's the only wildcard, the trailing substring will be extracted otherwise there is no extraction - the key is kept as is. TTL <code>ttl</code> Data time to live in seconds. Data doesn't expire if TTL is less than 1. By default, it's 0."},{"location":"stages-description/ReadWrite/#redshift","title":"Redshift","text":"UI Name Key (Code) Description Host <code>host</code> Host of the Redshift master node. Port <code>port</code> Port of the Redshift master node. Database <code>database</code> Identifies a Redshift database name. User <code>user</code> Credentials to access the database. Password <code>password</code> Credentials to access the database. SSL <code>ssl</code> Whether Redshift connection should be secured via SSL. Bucket <code>tempdir</code> A writeable location in Amazon S3, to be used for unloaded data when reading and Avro data to be loaded into Redshift when writing.If you're using Redshift data source for Spark as part of a regular ETL pipeline, it can be useful to set a Lifecycle Policy on a bucket and use that as a temp location for this data. Access key <code>accessKey</code> AWS access key, must have write permissions to the S3 bucket. Secret key <code>secretKey</code> AWS secret access key corresponding to provided access key. Extra copy options (optional) <code>extraCopyOptions</code> A list of extra options to append to the Redshift COPY command when loading data, e.g. TRUNCATECOLUMNS or MAXERROR n (see the Redshift docs for other options).Note that since these options are appended to the end of the COPY command, only options that make sense at the end of the command can be used, but that should cover most possible use cases. Custom SQL <code>customSql</code> Whether to read from custom query or from the table. Table <code>table</code> Database table to read/write. Query <code>query</code> Custom SQL query that will be loaded into DataFrame. <p>\"noAdditionalFields\": \"There are no additional fields.\",</p>"},{"location":"stages-description/ReadWrite/#stdout","title":"Stdout","text":"UI Name Key (Code) Description Description <code>description</code> The results can be seen in the logs. Quantity <code>quantity</code> Number of rows that will be displayed in the logs.The default value is 10.The minimum value is 1.The maximum value is 2147483631."},{"location":"stages-description/ReadWrite/#cluster","title":"Cluster","text":"UI Name Key (Code) Description File Path <code>filePath</code> File path. File Name <code>fileName</code> File name. File format <code>format</code> Spark DataFrame format.For CSV, please, specify Header and Delimiter. avroSchema <code>avroSchema</code> Avro can be used to define the data schema for a record's value.This schema describes the fields allowed in the value, along with their data types. Partition By (optional) <code>partitionBy</code> Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like:- year=2016/month=01/- year=2016/month=02/ File selection <code>fileSection</code> Path glob filter - It is used to only include files with file names matching the pattern.Recursive file lookup - Disables partition inferring.Modified before timestamp - Optional timestamp to only include files with modification times occurring before the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00).Modified after timestamp - Optional timestamp to only include files with modification times occurring after the specified time. The provided timestamp must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00). Compression codec <code>compressionCodec</code> Compression codec - Compression codec to use when saving to file. Binary format <code>binaryFormat</code> Binary format - Type of binary file to process.Output content column - Column to store parsed content as a text.Output path content - Column to store path to the parsed binary file. CSV format <code>csvFormatRead</code> Header - Column name.Delimeter - DelimeterSampling ratio - Defines fraction of input JSON objects used for schema inferring.Quote character - Sets a single character used for escaping quoted values where the separator can be part of the value.Escape character - Sets a single character used for escaping quotes inside an already quoted value.Schema inference - Infers the input schema automatically from data. It requires one extra pass over the data. CSV built-in functions ignore this option.Schema enforcement - If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored.Ignore leader white space - A flag indicating whether or not leading whitespaces from values being read/written should be skipped.Ignore trailing white space - A flag indicating whether or not trailing whitespaces from values being read/written should be skipped.Null value - Sets the string representation of a null value.Non-number value - Sets the string representation of a non-number value. CSV format <code>csvFormatWrite</code> Header - Column name.Delimeter - DelimeterCompression codec - Compression codec to use when saving to file.Quote character - Sets a single character used for escaping quoted values where the separator can be part of the value.Quote all - A flag indicating whether all values should always be enclosed in quotes. Default is to only escape values containing a quote character.Escape character - Sets a single character used for escaping quotes inside an already quoted value.Escape quotes - A flag indicating whether values containing quotes should always be enclosed in quotes. Default is to escape all values containing a quote character.Ignore leader white space - A flag indicating whether or not leading whitespaces from values being read/written should be skipped.Ignore trailing white space - A flag indicating whether or not trailing whitespaces from values being read/written should be skipped.Null value - Sets the string representation of a null value. Delta format <code>deltaFormatRead</code> Merge schema - Columns that are present in the DataFrame but missing from the table are automatically added as part of a write transaction when mergeSchema is true.Table version timestamp - Delta time travel to specific timestamp. Read version of the data at specific timestamp.Table version - Read specific version of the data. Delta format <code>deltaFormatWrite</code> Overwrite schema - By default, overwriting the data in a table does not overwrite the schema. When overwriting a table using mode('overwrite') without replaceWhere, you may still want to overwrite the schema of the data being written. You replace the schema and partitioning of the table by setting the overwriteSchema option to true.Replace where - Overwrite data matching a predicate over partition columns only.Max records per file - Specify the maximum number of records to write to a single file for a Delta Lake table. JSON format <code>jsonFormatRead</code> Sampling ratio - Defines fraction of input JSON objects used for schema inferring. JSON format <code>jsonFormatWrite</code> Compression codec - Compression codec to use when saving to file. ORC format <code>orcFormatRead</code> Merge schema - Columns that are present in the DataFrame but missing from the table are automatically added as part of a write transaction when mergeSchema is true. ORC format <code>orcFormatWrite</code> Compression codec - Compression codec to use when saving to file. Parquet format <code>parquetFormatRead</code> Merge schema - Columns that are present in the DataFrame but missing from the table are automatically added as part of a write transaction when mergeSchema is true. Parquet format <code>parquetFormatWrite</code> Compression codec - Compression codec to use when saving to file. Text format <code>textFormatRead</code> Whole text - If true, read each file from input path(s) as a single row. Text format <code>textFormatWrite</code> Compression codec - Compression codec to use when saving to file."},{"location":"stages-description/ReadWrite/#dataframe","title":"Dataframe","text":"UI Name Key (Code) Description Spark Dataframe <code>dataframe</code> Represent data as a Spark dataframe."},{"location":"stages-description/ReadWrite/#kafka","title":"Kafka","text":"UI Name Key (Code) Description Kafka Bootstrap Server URLs <code>bootstrapServers</code> Enter a comma-separated list of host and port pairs that are the addresses of the Kafka brokers in a \"bootstrap\" Kafka cluster that a Kafka client connects to initially to bootstrap itself. Topic Name <code>topicName</code> Provide the name of the topic you want to read events from in your Kafka data stream. Apache Kafka uses topics to organize and durably store events. You can use the drop-down list to select the topic from the Kafka data source. You can also use the field to search for the topic name. Certificate <code>filePath</code> Upload certificate. Options <code>options</code> Use 'Configure Options' button to set required key/value configuration."},{"location":"stages-description/ReadWrite/#api","title":"Api","text":"UI Name Key (Code) Description Host <code>host</code> API host is the domain name or IP address of the host that serves the API. Method <code>method</code> API method(GET, POST, PUT, DELETE, PATCH). Headers <code>headers</code> Use 'ADD HEADERS' button to set API headers in key/value format. Params <code>params</code> Use 'ADD PARAMS' button to set API parameters in key/value format."},{"location":"stages-description/ReadWrite/#databricks","title":"Databricks","text":"UI Name Key (Code) Description Catalog <code>catalog</code> Databricks Unity Catalog has a three-level namespace. Catalog is the first layer of the object hierarchy, used to organize you data assets. Provide a catalog name. Schema <code>schema</code> Name of the schema which is the second layer of the object hierarchy. It contains tables and views, or volumes. Object Type <code>objectType</code> Depending on the object type a user selects which type of data to work with. Use Table to access tabular data or Volume to work with non-tabular data stored in cloud object storage. Table <code>tableRead</code> Name of a table or a view that resides in the third layer of Unity Catalog's namespace. Merge schema <code>mergeSchema</code> Columns that are present in the DataFrame but missing from the table are automatically added as part of a write transaction when mergeSchema is true. Overwrite schema <code>overwriteSchema</code> By default, overwriting the data in a table does not overwrite the schema. When overwriting a table using mode('overwrite') without replaceWhere, you may still want to overwrite the schema of the data being written. You replace the schema and partitioning of the table by setting the overwriteSchema option to true. Table version timestamp <code>tableVersionTimestamp</code> Delta time travel to specific timestamp. Read version of the data at specific timestamp. Table version <code>tableVersion</code> Read specific version of the data. Replace where <code>replaceWhere</code> Overwrite data matching a predicate over partition columns only. Max records per file <code>maxRecordPerFile</code> Specify the maximum number of records to write to a single file for a Delta Lake table. Table <code>tableWrite</code> Name of a table that resides in the third layer of Unity Catalog's namespace. Path <code>path</code> A volume resides in the third layer of Unity Catalog's namespace. Volumes are siblings to tables, views, and other objects organized under a schema. They contain directories and files for data stored in numerous formats. Provide a path to a volume including its name and file name (/). Volume <code>volume</code> Volume name. File format <code>format</code> Spark DataFrame format.For CSV, please, specify Header and Delimiter. avroSchema <code>avroSchema</code> Avro can be used to define the data schema for a record's value.This schema describes the fields allowed in the value, along with their data types. Partition by <code>partitionBy</code> Partitions the output by the given columns on the file system. If specified, the output is laid out on the file system similar to Hive's partitioning scheme. As an example, when we partition a dataset by year and then month, the directory layout would look like:- year=2016/month=01/- year=2016/month=02/"},{"location":"stages-description/ReadWrite/#databricksjdbc","title":"Databricksjdbc","text":"UI Name Key (Code) Description JDBC URL <code>JDBCURL</code> A database connection URL is a string that your DBMS JDBC driver uses to connect to a database.The connection URL format for the driver is:jdbc:database-type://hostname:port[;property=value[;...]]. User <code>user</code> User name for a JDBC connection. Password <code>password</code> Password for a JDBC connection. Catalog <code>catalog</code> Databricks Unity Catalog has a three-level namespace. Catalog is the first layer of the object hierarchy, used to organize you data assets. Provide a catalog name. Schema <code>schema</code> Name of the schema which is the second layer of the object hierarchy. It contains tables and views, or volumes. Table <code>tableRead</code> Name of a table or a view that resides in the third layer of Unity Catalog's namespace. Table <code>tableWrite</code> Name of a table that resides in the third layer of Unity Catalog's namespace. Session init statement <code>sessionInitStatement</code> After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option(\"sessionInitStatement\", \"\"\"BEGIN execute immediate \"alter session set \"_serial_direct_read\"=true\"; END;'\"') Partition column, Lower bound, Upper bound <code>partitionColumn</code> These options must all be specified if any of them is specified. In addition, numPartitions must be specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric, date, or timestamp column from the table in question. Notice that lowerBound and upperBound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading. Number of partitions <code>numberOfpartitions</code> The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before writing. Fetch size <code>fetchSize</code> The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows). Batch size <code>batchSize</code> The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. Create table options <code>createTableOptions</code> This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.). Create table column types <code>createTableColumnTypes</code> The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: \"name CHAR(64), comments VARCHAR(1024)\"). The specified types should be valid spark sql data types. Custom schema <code>customSchema</code> The custom schema to use for reading data from JDBC connectors. For example, \"id DECIMAL(38, 0), name STRING\". You can also specify partial fields, and the others use the default type mapping. For example, \"id DECIMAL(38, 0)\". The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults. SQL statement <code>sqlStatement</code> Displays the schema and the table fields, if you choose false. If you choose true, you will be able to write your own SQL code in the provided field."},{"location":"stages-description/cache/","title":"Cache","text":"<p>Persists the data set in some storage. The storage type can be tweaked by specifying/combining parameters.</p> <p>Overall, the configuration gives the ability to define:</p> <ol> <li> <p>Whether to use memory.</p> </li> <li> <p>Whether to drop the RDD to disk if it falls out of memory.</p> </li> <li> <p>Whether to keep the data in memory in a serialized format.</p> </li> <li> <p>Whether to replicate the RDD partitions on multiple nodes.</p> </li> </ol> <p>By default the cache parameters are set to reflect StorageLevel.MEMORY_AND_DISK mode.</p>"},{"location":"stages-description/cache/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Use disk <code>useDisk</code> Save data on disc. Use memory <code>useMemory</code> Data should be saved in RAM. Use off heap <code>useOffHeap</code> Objects will be managed by garbage collector. Deserialized <code>deserialized</code> Objects should be automatically serialized. Replication <code>replication</code> Replicate the RDD partitions on multiple nodes."},{"location":"stages-description/cdc/","title":"Cdc","text":"<p>This stage is intended to find all differences between before (old) and after (new) datasets.</p> <p>Based on differences CDC produces additional column 'Operation', which indicates the state of the row from the old dataset considering it's presence/absence in the new one.</p> <p>CDC compares each row of the new and the old datasets, based on key and columns to compare values and sets Operation value.</p> <p>NOTE: old and new datasets must not contain duplicates (rows with the same key) based on key column(s).</p> <p>Old and new datasets columns to compare and key columns must be presented in both datasets with the same names. If there are duplicated rows at least in one of dataset, results of the CDC will be unpredictable.</p> <p>CDC change codes (column Operation in the result set):</p> <p>0 - Copy (row doesn't have any changes)</p> <p>1 - Insert (row exists only in new dataset, should be inserted)</p> <p>2 - Delete (row exists only in old dataset, should be deleted)</p> <p>3 - Update (row exists and has different values in both datasets, should be updates in old with new values from new)</p>"},{"location":"stages-description/cdc/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Link Ordering <code>linkOrdering</code> This option allows you to specify which input link carries the before data set and which carries the after data set. To rearrange the links, click the up arrow button or the down arrow button. Key <code>key</code> Specify unique key to compare. As example, 1 or more columns. Columns names should be the same. Mode <code>mode</code> Return All. Returns all rows (modified and unmodified).Return Delta. Returns only modified rows."},{"location":"stages-description/container/","title":"Container","text":""},{"location":"stages-description/container/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Image link <code>image</code> Docker image path. Examples: mysql, mysql:latest, bitnami/argo-cd:2.1.2, localhost:5000/bitnami/argo-cd:2.1.2, registry.redhat.io/rhel7:latest. Image pull policy <code>imagePullPolicy</code> Defines when the image will be pulled(downloaded). Possible values:If not present - download only if not exist locally;Always - download before each start;Never - do not download use only local copy. Requests CPU <code>requestsCpu</code> Minimal number of CPU cores reserved for this container. Requests memory <code>requestsMemory</code> Minimal amount of RAM reserved for this container. Limits CPU <code>limitsCpu</code> Maximum limit of CPU cores that container can use. Limits memory <code>limitsMemory</code> Maximum limit of RAM that container can use. Mount project params <code>mountProjectParams</code> Defines whether to mount all project params as environment variables inside the Pod. Authentication type <code>imagePullSecretType</code> Authentication mode that could be one of these:1) Not applicable - image pull secrets are not needed, as the image is pulled from the public registry;2) New - create a new image pull secret on the fly by providing all necessary information (see below);3) Provided - use existing image pull secret by providing it's name (Image pull secret name). Image pull secret name <code>imagePullSecretName</code> Name of the secret to pull the image. Note that it must exist within the same k8s namespace as the current pipeline. Username <code>username</code> User name for registry authentication. Password <code>password</code> Password for registry authentication. Registry <code>registry</code> Name of the registry for authentication. Command <code>command</code> Command that will be executed once Pod is be created."},{"location":"stages-description/datetime/","title":"Datetime","text":""},{"location":"stages-description/datetime/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Choose operation type <code>chooseOperation</code> Operation Type <code>operationType</code> Operation types possible values description:"},{"location":"stages-description/datetime/#option","title":"Option","text":"UI Name Key (Code) Description targetColumn <code>targetColumn</code> Column where the result is displayed. sourceColumn <code>sourceColumn</code> Column from which data is taken. format <code>format</code> Format specified by the date format.Format: 'year', 'yyyy', 'yy' to truncate by year, 'month', 'mon', 'mm' to truncate by month.format: 'year', 'yyyy', 'yy' to truncate by year, 'month', 'mon', 'mm' to truncate by month, 'day', 'dd' to truncate by day. Other options are: 'second', 'minute', 'hour', 'week', 'month', 'quarter'.Format: yyyy-MM-dd HH:mm:ss. numMonths <code>numMonths</code> Number of months. days <code>days</code> Number of days. roundOff <code>roundOff</code> If roundOff is set to true, the result is rounded off to 8 digits; it is not rounded otherwise. dayOfWeek <code>dayOfWeek</code> Day of the week; For example: Sunday. startColumn <code>startColumn</code> Start date column. endColumn <code>endColumn</code> End date column."},{"location":"stages-description/datetime/#operationtypes","title":"Operationtypes","text":"UI Name Key (Code) Description current_date <code>current_date</code> Returns the current date as a date column. date_format <code>date_format</code> Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. to_date <code>to_date</code> Convert string type containing date value to date format. add_months <code>add_months</code> Add months to date. date_add <code>date_add</code> Add days to the date. date_sub <code>date_sub</code> Subtract the days from date field. datediff <code>datediff</code> Returns difference between two dates in days. months_between <code>months_between</code> Returns number of months between two dates. next_day <code>next_day</code> Returns the first date which is later than the value of the date column. year <code>year</code> Extract the year of a given date as integer. quarter <code>quarter</code> Extract the quarter of a given date as integer. month <code>month</code> Extract the month of a given date as integer. dayofweek <code>dayofweek</code> Returns the first date which is later than the value of the date column that is on the specified day of the week. dayofmonth <code>dayofmonth</code> Extracts the day of the month as an integer from a given date/timestamp/string. dayofyear <code>dayofyear</code> Extracts the day of the year as an integer from a given date/timestamp/string. weekofyear <code>weekofyear</code> Extracts the week number as an integer from a given date/timestamp/string. A week is considered to start on a Monday and week 1 is the first week with more than 3 days, as defined by ISO 8601. last_day <code>last_day</code> Returns the last day of the month which the given date belongs to. trunc <code>trunc</code> Returns date truncated to the unit specified by the format. current_timestamp <code>current_timestamp</code> Returns the current timestamp as a timestamp column. hour <code>hour</code> Extracts the hours as an integer from a given date/timestamp/string. minute <code>minute</code> Extracts the minutes as an integer from a given date/timestamp/string. second <code>second</code> Extracts the seconds as an integer from a given date/timestamp/string. to_timestamp <code>to_timestamp</code> Converts time string with the given pattern to timestamp. date_trunc <code>date_trunc</code> Returns timestamp truncated to the unit specified by the format. unix_timestamp <code>unix_timestamp</code> Returns the current Unix timestamp (in seconds) as a long. to_unix_timestamp <code>to_unix_timestamp</code> Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp. from_unixtime <code>from_unixtime</code> Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format."},{"location":"stages-description/filter/","title":"Filter","text":""},{"location":"stages-description/filter/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Filter <code>filter</code> Enter any boolean expression. Two or more expressions may be combined together using the logical  operators ( AND, OR ).Examples:&gt; ((column1 &lt; 10) and column2 between 10 and 25)More examples"},{"location":"stages-description/groupBy/","title":"Groupby","text":""},{"location":"stages-description/groupBy/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Drop grouping columns <code>dropGroupingColumns</code> Select checkbox to remove grouped columns from the output. Group By <code>groupBy</code> Specify the key column for the operation. To specify more than one key, use a comma or Enter. Aggregate functions <code>aggregateFunctions</code> Specify aggreagate functions:1. Click on the Plus button.2. In the drop-down lists that appear, specify the function and column name.Available functions: Count, Avg, Min, Max, Sum, Mean.Notes:1. Use the Plus button to add as many columns as you need.2. Use the Minus button to remove the filter.3. Use the Upp Arrow and Down Arrow to change the order of columns.For more examples click here."},{"location":"stages-description/handleNull/","title":"Handlenull","text":""},{"location":"stages-description/handleNull/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Mode <code>mode</code> Provide mode:1. Drop - Allows to remove rows/column with null values.2. Fill - Allows to replace null values. Drop Type <code>dropType</code> Replacement type for missing values (\"Column\" | \"Row\").\"Column\" option allows to remove all or specific columns where all the rows have null values.\"Row\" option allows to remove all rows, where all the values are null. Drop Choice <code>dropChoice</code> What to drop:1.\"Names\"- remove all rows, where all the values are null in some column (works as a filter).2.\"All\". Drop Columns <code>dropColumns</code> Which columns to delete if Drop Choice = \"Names\" . Fill Value Type <code>fillValueType</code> Replacement type for missing values (\"Custom\" | \"Agg\").Notes:Set aggregation function in \"Fill Strategy\" field. Fill Choice <code>fillChoice</code> What to fill:1.\"Names\"- provide specific column(s) and value for it in \"Fill nulls\" section.2.\"All\".Notes:Use the Plus button to add Column/Value pairs. Fill Values <code>fillValues</code> Value(s) for columns to replace nulls. Fill Columns <code>fillColumns</code> Column(s) to be replaced by using aggregate function Fill Strategy <code>fillStrategy</code> Aggregation type (\"mean\" | \"median\" | \"mode\")"},{"location":"stages-description/job/","title":"Job","text":""},{"location":"stages-description/job/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Stage Name <code>name</code> Provide Stage name. Job Name <code>job</code> Select one of the existing jobs."},{"location":"stages-description/join/","title":"Join","text":""},{"location":"stages-description/join/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Join type <code>joinType</code> Inner join. Transfers records from input data sets whose key columns contain equal values to the output data set. Records whose key columns do not contain equal values are dropped.Left outer join. Transfers all values from the left data set but transfers values from the right data set only where key columns match. The stage drops the key column from the right data set.Right outer join. Transfers all values from the right data set and transfers values from the left data set and intermediate data sets only where key columns match. The stage drops the key column from the left and intermediate data sets.Full outer join. Transfers records in which the contents of the key columns are equal from the left and right input data sets to the output data set. It also transfers records whose key columns contain unequal values from both input data sets to the output data set.Cross join. Returns a result data set where each row from the first table is combined with each row from the second table.Left semi join. Returns values from the left side of the relation that has a match with the right.Left anti join. Returns values from the left relation that has no match with the right. Link Ordering <code>linkOrdering</code> This option allows you to specify which input link is regarded as the left link and which link is regarded as the right link. By default the first link you add is regarded as the left link, and the last one as the right link. You can use arrow up and arrow down buttons to override the default order. Key <code>key</code> Specify keys to combine rows from multiple tables."},{"location":"stages-description/notification/","title":"Notification","text":""},{"location":"stages-description/notification/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. To <code>to</code> Select one of the existing parameters that related to email or enter slack email. Message <code>message</code> Enter the text that will be sent to the selected/entered email."},{"location":"stages-description/pipeline/","title":"Pipeline","text":""},{"location":"stages-description/pipeline/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Stage Name <code>name</code> Provide Stage name. Pipeline Name <code>pipeline</code> Select one of the existing pipelines."},{"location":"stages-description/pivot/","title":"Pivot","text":""},{"location":"stages-description/pivot/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Operation Type <code>operationType</code> Provide operation type:1. pivot - Transpose data from one column into multiple columns.2. unpivot - Transpose data from multiple columns to one column. Group By <code>groupBy</code> Column(s) for grouping. Aggregate function and column <code>aggregateFunc</code> Aggregate function and column. Pivot Values <code>pivotValues</code> Value(s) from pivot column to be used (optional). Pivot Columns <code>pivotColumns</code> Columns for pivoting. Unchanged Columns <code>unchangedColumns</code> Unchanged column(s). Unpivot Columns <code>unpivotColumns</code> Columns for unpivoting. Unpivot Names <code>unpivotNames</code> Names for columns after unpivoting."},{"location":"stages-description/removeDuplicates/","title":"Removeduplicates","text":""},{"location":"stages-description/removeDuplicates/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Key <code>key</code> Specify the key column for the operation. To specify more than one key, use a comma or Enter. Order By <code>orderBy</code> You can specify the order of output columns:1. Click on the Plus button.2. In the drop-down lists that appear, specify the sort order (Asc or Desc) and column name.Notes:1. Use the Plus button to add as many columns as you need.2. Use the Minus button to remove the filter.3. When multiple columns are used in ORDER BY, first the rows will be sorted based on the firstcolumn and then by the second column. Use the Upp Arrow and Down Arrow to change the order of columns.\ud835\udc08\ud835\udc26\ud835\udc29\ud835\udc28\ud835\udc2b\ud835\udc2d\ud835\udc1a\ud835\udc27\ud835\udc2d: This parameter is used within the process of removing duplicates, it does not affect the result set."},{"location":"stages-description/slice/","title":"Slice","text":""},{"location":"stages-description/slice/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide a stage name. Mode <code>Mode</code> Slice mode defines the type of behavior, whether it drops columns or keep them. 'Drop mode' \u2014 All mentioned columns will be dropped from the result. 'Keep mode' \u2014 All mentioned columns will be kept in the result. Columns <code>Columns</code> Specify column names."},{"location":"stages-description/sort/","title":"Sort","text":""},{"location":"stages-description/sort/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Sort type <code>sortType</code> Choose type of sort (Full sort or Sort within partitions).Full sort sorts DataFrame by the specified column(s).Sort within partitions sorts each DataFrame partition by the specified column(s).The order of the output data is not guaranteed because the data is ordered on partition-level. Order columns <code>orderColumns</code> Choose column(s) and sorting order (default value is asc).Possible sorting orders: asc, asc nulls first, asc nulls last, desc, desc nulls first, desc nulls last."},{"location":"stages-description/stringFunctions/","title":"Stringfunctions","text":""},{"location":"stages-description/stringFunctions/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide a stage name. Operation Type <code>operationType</code> Select function type. Source Column <code>sourceColumn</code> Column from which data is taken. Target Column <code>targetColumn</code> Column where the result is displayed. Separator <code>separator</code> Separator between string input values. Decimal Places <code>decimalPlaces</code> A number of simbols after comma. Substring searched for <code>substringSearch</code> Substring that is searched for in the sourceColumn. Position <code>position</code> Position at which to start searching for a substring. Length <code>length</code> Length of targetColumn. Limit <code>limit</code> Limit of splitting. Delimiter <code>delimiter</code> Delimiter strting for searching. Count <code>count</code> Count occurrences of the delimiter."},{"location":"stages-description/transformer/","title":"Transformer","text":""},{"location":"stages-description/transformer/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Output <code>output</code> Transformer stage gives the user an ability to modify columns that will be written to some data storage later on.The customization allows for (and not limited to):1. Specify only needed columns. 2. Provide alias for columns.3. Use spark-sql functions/procedures to modify (or create new) columns.For more information about Built-in Functions click here. Note <code>note</code> An identifier is a string used to identify a database object such as a table, view, schema, column, etc. Spark SQL has regular identifiers and delimited identifiers, which are enclosed within backticks. Therefore be vigilant when you reference certain database objects in your query since some might have symbols that might be misinterpreted (e.g. spark function invocation) if the whole object's name is not guarded byFor more information of backticks click here. Mode <code>mode</code> Transformer mode defines the type of the SQL query (Spark SQL dialect) that is accepted and executed.Simple - only allows you to specify the part between SELECT and FROM. You can do things like these:1) col1, concat(col1, col2)2) count(*) as count3) a, b, row_number() OVER (PARTITION BY a ORDER BY b)4) col, exists(col, x -&gt; x % 2 == 0)5) col, collect_list(col)Syntax:  as , function() as Full SQL - allows you to write a full-blown Spark SQL query. Though you would have to specify the table's name in the query by manually referencing the value from the \"Table name\" parameter. Table name <code>tableName</code> The name of the table that you should use within the Spark SQL query. Only applicable for Full SQL transformer mode."},{"location":"stages-description/union/","title":"Union","text":"<p>Column's sequence, names, types are important.</p>"},{"location":"stages-description/union/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Mode <code>mode</code> Distinct values. Returns result without any duplicate rows.All values. Returns the result, including duplicates."},{"location":"stages-description/validation/","title":"Validation","text":""},{"location":"stages-description/validation/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. isError <code>isError</code> End job with error if validation fails. Schema <code>validationSchema</code> Provide validation schema. Column <code>column</code> Specify column name. dataType <code>dataType</code> Data type of column.Note: may work incorrectly when reading files without a schema (when reading \u0441sv files, all fields have string type). minValue <code>minValue</code> Minimum field value. maxValue <code>maxValue</code> Maximum field value. minLength <code>minLength</code> Minimum string length. maxLength <code>maxLength</code> Maximum string length. inValues <code>inValues</code> Possible values separated by commas. regex <code>regex</code> Regular expression. notNull <code>notNull</code> Doesn't contain null value. uniqueness <code>uniqueness</code> Contains only unique values."},{"location":"stages-description/wait/","title":"Wait","text":""},{"location":"stages-description/wait/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Wait for stages to be completed before proceeding. <code>value</code>"},{"location":"stages-description/withColumn/","title":"Withcolumn","text":"<p>'Add/Update Column' stage gives the user an ability to perform the following operations with data:</p> <ol> <li> <p>Adding a new column to dataframe.</p> </li> <li> <p>Changing value of an existing column. </p> </li> <li> <p>Deriving new column from an existing column.</p> </li> <li> <p>Using conditions to provide column values.</p> </li> <li> <p>Changing column data type.</p> </li> <li> <p>Working with Window Functions.</p> </li> <li> <p>Replacing column values that match the pattern with new ones, replace column values character by character, using conditions.</p> </li> </ol>"},{"location":"stages-description/withColumn/#configuration-parameters","title":"Configuration Parameters","text":"UI Name Key (Code) Description Name <code>name</code> Provide Stage name. Operation Type <code>operationType</code> 'deriveColumn' \u2014 create a new column from the existing one. 'addConstant' \u2014 create a column with constant values. 'changeType' \u2014 change column type. 'renameColumn' \u2014 rename the column. 'useConditions' \u2014 create a column with values that depend on some conditions. 'useWindowFunction' \u2014 create a column with values obtained as a result of using the window function. 'replaceValues' \u2014 replace column values that match the pattern with new ones. 'replaceValuesUsingConditions' \u2014 replace values in the column using conditions. 'replaceValuesCharByChar' \u2014 replace column values character by character. Column <code>column</code> Specify column name."},{"location":"user-guide/introduction/process-overview/","title":"Process Overview","text":"<p>Visual Flow jobs and pipelines exist within a certain namespace (project), so the first step in the application would be to create a project or enter an existing one. Then you need to enter Job Designer to create a job.</p> <p>Job Designer is a graphical user interface used to create, maintain, execute and analyze jobs. Each job determines the data sources, the required transformations and the destination of data.</p> <p>Pipeline designer is a graphical user interface aimed at managing pipelines. Designing a pipeline is similar to doing so on a job.</p> <p>Important: When editing stages in the Configuration Panel, to save data a user must click the Confirm button, otherwise the data will be lost.</p> <p>Visual Flow key functions include but not limited to:</p> <ul> <li>Create a project which serves as a namespace for jobs and/or pipelines</li> <li>Manage project settings</li> <li>User access management</li> <li>Create/maintain a job in Job Designer</li> <li>Job execution and logs analysis</li> <li>Create/maintain a pipeline in Pipeline Designer</li> <li>Pipeline execution</li> <li>Import/Export jobs and pipelines</li> </ul>"},{"location":"user-guide/introduction/scope-and-purpose/","title":"Scope and Purpose","text":"<p>The Visual Flow web application is the ETL/ELT tool designed for effective data management via a convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li> <p>Can integrate data from heterogeneous sources:</p> <ul> <li>Azure Blob Storage</li> <li>API</li> <li>AWS S3</li> <li>Cassandra</li> <li>ClickHouse</li> <li>Dataframe</li> <li>Databricks JDBC (Global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>DB2</li> <li>Google Cloud Storage</li> <li>Elasticsearch</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>Mongo</li> <li>MSSQL</li> <li>MySQL</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>Redshift-jdbc </li> </ul> </li> <li> <p>It supports the following file formats:</p> <ul> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> </ul> </li> <li> <p>Leverage direct connectivity to enterprise applications as sources and targets</p> </li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allow to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverage Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul>"},{"location":"user-guide/introduction/terminology/","title":"Terminology","text":""},{"location":"user-guide/introduction/terminology/#etl-extract-transform-load","title":"ETL (Extract, Transform, Load)","text":"<p>ETL is an abbreviation for extract, transform, load \u2014 three database functions combined into one tool to pull data out of one database, transform it and place into another.</p> <ul> <li>Extract is the process of reading data from a database. In this stage, the data is collected, often from multiple and different types of sources.</li> <li>Transform is the process of converting the extracted data from its previous form into the form needed to place it into another database.</li> <li>Load is the process of writing the data into the target database.</li> </ul>"},{"location":"user-guide/introduction/terminology/#job","title":"Job","text":"<p>A job is a chain of individual stages linked together. It describes the flow of data from its source to target. Usually, a job has a minimum of one data input and output. However, some jobs can accept more than one data input and output it to more than one target. In Visual Flow, the available stages are:</p> <ul> <li>Read</li> <li>Write</li> <li>Group By</li> <li>Remove duplicates</li> <li>Filter</li> <li>Transformer</li> <li>Sort</li> <li>Slice</li> <li>Pivot</li> <li>Add/Update Column</li> <li>String Functions</li> <li>Date/Time</li> <li>Drop/Fill Nulls</li> <li>Join</li> <li>Union</li> <li>Change Data Capture</li> <li>Cache</li> <li>Validate</li> </ul>"},{"location":"user-guide/introduction/terminology/#pipeline","title":"Pipeline","text":"<p>A pipeline is a compound of multiple jobs and can be run. In Visual Flow, the user can use such stages as:</p> <ul> <li>Job</li> <li>Container</li> <li>Notification</li> <li>Wait</li> </ul>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/add-update-column/","title":"Add/Update Column Stage.","text":"<p>The Add/Update Column stage is used to add a new column to a dataframe, change value of an existing column, con- vert the data type of a column or derive a new column from an existing column. Also it gives the ability to use condi- tions or window functions to provide column values. It has the following operations types:</p> <ul> <li>deriveColumn</li> <li>addConstant</li> <li>changeType</li> <li>renameColumn</li> <li>useConditions</li> <li>useWindowFunctions</li> </ul> <p>If you select useWindowFunctions you need to pick a function from the dropdown list. Each one has its own parame- ters to enter.</p> <p>The available window functions are:</p> <ul> <li>rank</li> <li>dense_rank</li> <li>percent_rank</li> <li>cume_dist</li> <li>row_number</li> <li>ntile</li> <li>lag</li> <li>lead</li> <li>count</li> <li>sum</li> <li>avg</li> <li>max</li> <li>min</li> </ul>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/cache/","title":"Cache Stage.","text":"<p>The Cache stage persists dataset in some storage. You can tweak the storage type by specifying parameters.</p> <p>The configuration gives you the ability to define:</p> <ul> <li>Whether to use memory.</li> <li>Whether to drop the RDD to disk if it falls out of memory.</li> <li>Whether to keep the data in memory in a serialized format.</li> <li>Whether to replicate the RDD partitions on multiple nodes.</li> </ul> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/change-data-capture/","title":"Change Data Capture Stage.","text":"<p>This stage is intended to find all differences between before (old) and after (new) datasets. Based on them, CDC produces an additional column \u2014 'Operation', which indicates the state of the row from the old dataset considering its presence/absence in the new one. CDC compares each row of the new and the old datasets based on keys and col- umns to compare values and sets Operation value.</p> <p>Note: old and new datasets must not contain duplicates (rows with the same key) based on key column(s). Old and new datasets columns to compare, as well as key columns, must be present in both datasets with the same name. If there are duplicated rows in at least one dataset, the result of the CDC gets unpredictable.</p> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/date-time/","title":"Date/Time Stage.","text":"<p>Date/Time stage leverages Spark standard built-in date and timestamp functions which come in handy when we need to make operations on date and time. </p> <p>All these operations accept input of type: date, timestamp or string.  If string, it should be in a format that can be cast to date, such as <code>'yyyy-MM-dd'</code>  and timestamp in <code>'yyyy-MM-dd'</code> <code>'HH:mm:ss.SSSS'</code>.  It returns date and timestamp respectively. </p> <p>Null is returned if the input data was a string that could not be cast to date or timestamp.</p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/drop-fill-nulls/","title":"Drop/Fill Nulls Stage.","text":"<p>Drop/Fill Null stage allows gracious null handling. There are 2 modes:</p> <ul> <li>Drop</li> <li>Fill</li> </ul> <p>With Drop mode you can:</p> <p>1) Remove all columns where all the rows have null values. For that you need to set Drop Type to Column and Drop Choice to All.</p> <p>2) Remove specific columns where all the rows have null values. For that you need to set Drop Type to Column, Drop Choice to Names and specify column names you want to drop.</p> <p>3) Remove all rows where all the values are null. For that you need to set Drop Type \u2013 Row and Drop Choice \u2013 All.</p> <p>4) Remove all rows, where all the values are null in specific columns. (Works as a filter). For that you need to set Drop Type to Row, Drop Choice to Names and specify drop column names.</p> <p>Fill mode is used to replace NULL values on a dataframe column with any constant value or using aggregate function. ( Fill Value Type - Custom or Agg respectively ). Fill Value Type - Custom allows you to replace all columns with one value at once or specify a specific value per column you want to fill with. For that you need to select Fill Choice - All or Names respectively. Fill Value Type - Agg allows you to fill a specific column with a Strategy, which stands for the aggregation types: \"mean\", \"median\" or \"mode\".</p> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/filter/","title":"Filter Stage.","text":"<p>Enter any boolean expression. You can combine two or more ones by using logical operators (AND, OR). Examples: (column1 &lt; 10) and (column2 between 10 and 25).</p> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/group-by/","title":"Group By Stage.","text":"<p>The stage allows grouping by columns, which must be specified in the Configuration panel. There is an option Drop grouping columns for removing them from the output. Also you can use the aggregate function, e.g., Count, Avg, etc.</p> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/join/","title":"Join Stage.","text":"<p>Available types of join are:</p> <ul> <li>Inner join. Transfers records from input data sets whose key columns contain equal values to the output data set. Records whose key columns do not contain equal values are dropped.</li> <li>Left outer join. Transfers all values from the left data set but transfers them from the right one only when key columns match. The stage drops the key column from the right data set.</li> <li>Right outer join. Transfers all values from the right and left data sets. Intermediate them only when key columns match. The stage drops the key column from the left and intermediate data sets.</li> <li>Full outer join. Transfers records in which the contents of the key columns are equal from the left and right input data sets to the output. It also transfers records whose key columns contain unequal values from input and output data sets.</li> <li> <p>Cross join. Returns a result data set where each row from the first table is combined with each from the second table.</p> </li> <li> <p>Left semi join. Returns values from the left relation that has a match with the right.</p> </li> <li>Left anti join. Returns values from the left relation that has NO match with the right.</li> </ul> <p>Link Ordering option allows you to specify which input link is regarded as the left and which as the right. By default, the first link added is left, and the last one is right. With Left Key and Right Key you must enter the key of joining.</p> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/pivot/","title":"Pivot","text":"<p>Pivot Stage. The Pivot stage has two operation types:</p> <ul> <li>Pivot</li> <li>Unpivot</li> </ul> <p>Pivot function rotates data from one column into multiple columns (transpose a row to a column). With aggregation one of the grouping column values is transposed into individual columns with distinct data.</p> Product Amount Country Banana 1000 USA Carrots 1500 USA Beans 1600 USA Orange 2000 USA Orange 2000 USA Banana 400 China Carrots 1200 China Beans 1500 China Orange 4000 China Banana 2000 Canada Carrots 2000 Canada Beans 2000 Mexico <p>From the above dataframe, to get the total amount exported to each country of each product we group by Product, pivot by Country and sum the Amount. This transposes the countries from the dataframe rows into columns and pro- duces the below output. Missing data is represented as Null.</p> Product Canada China Mexico USA Orange Null 4000 Null 4000 Beans Null 1500 2000 1600 Banana 2000 400 Null 1000 Carrots 2000 1200 Null 1500 <p>Unpivot is a reverse operation used to transform it back by rotating column values into rows values. For Pivot operation you need to specify:</p> <ul> <li>Group By. Columns for grouping</li> <li>Pivot columns</li> <li>Aggregate function and column</li> <li>Pivot values. Values from pivot column to be used (optional)</li> </ul> <p>For Unpivot operation you need to specify:</p> <ul> <li>Unchanged columns</li> <li>Unpivot columns. Columns for unpivoting</li> <li>Unpivot names. Column names for unpivoting</li> </ul>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/read/","title":"Read Stage.","text":"<p>You can start creating a job by dragging a stage to the canvas, e.g., you can drag the Read stage:</p> <p></p> <p>Note: you can also add a stage by double clicking its tile on the palette. Double-click on a stage on canvas opens the configuration panel on the right:</p> <p></p> <p>Enter a name for the stage and select storage e.g. DB2 if you want to read data from the DB2 table.</p> <p>Available Storage values for Read stage are:</p> <ul> <li>API</li> <li>AWS S3</li> <li>Cassandra</li> <li>ClickHouse</li> <li>Dataframe</li> <li>DB2</li> <li>Elasticsearch</li> <li>IBM COS</li> <li>Local file</li> <li>Mongo</li> <li>MySQL</li> <li>MSSQL</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>Redshift-jdbc</li> </ul> <p>Important: you can select an existing connection with the Parameters button near the Storage field:</p> <p></p> <p>If you do so, its parameters, e.g., JDBC URL, user, etc., are filled automatically. But now, suppose you don\u2019t have a connection created previously, so fill required parameters for DB2 storage manually:</p> <p></p> <p>Important: you can pick up a parameter value with the Parameters  button on the right panel if it is previously created as a project parameter.</p> <p></p> <p>The Read stage for DB2 storage has an option Custom SQL to read data with SQL statement (e.g., select * from table where field = value). If you select Custom SQL - True you need to enter the SQL statement and specify the schema:</p> <p></p> <p>For the Redis source you need to define Key column, Model, SSL, Read mode, Keys pattern or Table fields in the Configuration of the Read stage.</p> <ul> <li>Key column. The Column name to store the hash key.</li> <li>Model (binary, hash) defines the Redis model used to persist dataframe. By default, it is hash.</li> <li>Read mode (key, pattern) defines how the read operation is handled. If Key is selected, then the read is done based on the table field. In the case of Pattern the provided pattern (option Keys Pattern) dictates what Redis key to read. Keys pattern. If the pattern ends with * (e.g., \u201ctable: *\u201d), all keys from it are read. If one pattern is defined (e.g., \u201ctable: first value\u201d), then only one key is read.</li> </ul> <p></p> <p>Note: with Dataframe storage option you can create your own dataset:</p> <p></p> <p>Save the stage by pushing the Confirm button on the configuration panel. Push the Save button above if you want to save the job at this step. When the first stage of the job is configured, the canvas looks like this:</p> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/remove-duplicates/","title":"Remove Duplicates Stage.","text":"<p>Specify a key column for the operation. To specify more than one key, use a comma or Enter. For Order By operation, you need to select columns to sort by and sort order. The default is ascending.</p> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/slice/","title":"Slice Stage.","text":"<p>The Slice stage allows you to remove unnecessary columns from your data stream. There are 2 modes of Slice stage:</p> <ul> <li>Keep</li> <li>Drop</li> </ul> <p>The default mode is Drop. In this case, you specify columns you would like to slice from your data flow. With Keep mode selected, you specify the columns you want to keep.</p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/sort/","title":"Sort Stage.","text":"<p>There are two types of sorting available: Full sort and Sort within partitions. Full sort classifies dataframe by the specified column(s). Sort within partitions sorts each dataframe partition by the specified column(s). In this case, the order of the output data is not guaranteed because the data is ordered at the partition level. Select column(s) to sort by and the sort order (default value is Asc). Available sort options:</p> <ul> <li>asc</li> <li>asc nulls first</li> <li>asc nulls last</li> <li>desc</li> <li>desc nulls first</li> <li>desc nulls last</li> </ul>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/string-functions/","title":"String Functions Stage.","text":"<p>The String Functions stage is handy when we need to make string operations on a dataframe column. The Operation type dropdown list contains the following Spark built-in standard string functions:</p> <ul> <li>ascii</li> <li>base64</li> <li>concat_ws</li> <li>decode</li> <li>encode</li> <li>format_number</li> <li>format_string</li> <li>title_case</li> <li>instr</li> <li>length</li> <li>lower</li> <li>locate</li> <li>lpad</li> <li>ltrim</li> <li>regexp_extract</li> <li>ubase64</li> <li>rpad</li> <li>repeat</li> <li>rtrim</li> <li>split</li> <li>substring</li> <li>substring_index</li> <li>trim</li> <li>upper</li> </ul>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/transformer/","title":"Transformer Stage.","text":"<p>The Transformer stage allows you to modify columns to fill them with data further. Transformer mode defines the type of SQL query (Spark SQL dialect) which is taken to execute. Simple mode only allows you to specify the part between SELECT and FROM. You can do things like these: <pre><code>col1, concat(col1, col2)\ncount(*) as count\na, b, row_number() OVER (PARTITION BY a ORDER BY b)\ncol, exists(col, x -&gt; x % 2 == 0)\ncol, collect_list(col)\n</code></pre></p> <p>The syntax is:  as , function() as . <p>Full SQL mode allows you to write a full-blown Spark SQL query. In this case, you have to specify a table name manually or reference a table name from a parameter.</p> <p>Table name. The name of the table which should be used within the Spark SQL query. Applicable for Full SQL transformer mode only.</p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/union/","title":"Union Stage.","text":"<p>You can union two datasets.</p> <p>Note: Columns sequence, names, and types are important for union operation.</p> <p>Mode contains 2 options: All values and Distinct values.</p> <ul> <li>Distinct values, which is the default, eliminates duplicate records from the second dataset.</li> <li>All values needs to be specified explicitly, and it tolerates duplicates from the second dataset.</li> </ul> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/validate/","title":"Validate Stage","text":"<p>With the Validate stage you can run the data validation using Spark job validation types:</p> <ul> <li>dataType</li> <li>minValue</li> <li>maxValue</li> <li>minLength</li> <li>maxLength</li> <li>notNull</li> <li>inValues</li> <li>regex</li> <li>uniqueness</li> </ul> <p>To add a validation you need to push Add Validation button on the stage configuration panel and start adding a column validation:</p> <p></p> <p>When you hit Ok the validation row appears:</p> <p></p> <p>Once you are done with adding all validations push Save on the dialog screen and then confirm the stage configuration. Save the job by pushing Save on Job Designer header. For a newly created job, as long as it is not yet run, its status is Draft: </p> <p>Drag other stages according to the data flow from source to destination. See the job with more stages as an example:</p> <p></p>"},{"location":"user-guide/job-operations/create-a-job-available-stages/available-stages/write/","title":"Write Stage.","text":"<p>Now drag another stage, e.g., Write stage:</p> <p></p> <p>Enter a name for the stage and select storage IBM COS if you want to post data from the DB2 table to the Cloud Object Storage file. Fill required parameters for the IBM COS storage.</p> <p>Available storage values for the Write stage are:</p> <ul> <li>AWS S3</li> <li>Cassandra</li> <li>ClickHouse</li> <li>DB2</li> <li>Elasticsearch</li> <li>IBM COS</li> <li>Local file</li> <li>Mongo</li> <li>MSSQL</li> <li>MySQL</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>Redshift-jdbc</li> <li>STDOUT</li> </ul> <p>IBM COS storage has two options of Authentication type: HMAC and IAM. If HMAC is selected, you should fill accessKey and secretKey. If IAM is selected, iamApiKey and iamServiceId should be entered.</p> <p>For the storages IBM COS and AWS S3 the function Partition By can be used in the Write stage. It partitions the output on the file system by given columns. If specified, the output is laid out on the file system similar to Hive's partitioning scheme.</p> <p>As an example, when we partition a dataset by year and then month, the directory layout looks like this: - year=2016/month=01/ - year=2016/month=02/ In the case of importing table data with Write stage to Cassandra source from another storage, the table layout for output must be previously created in Cassandra. Columns, key fields, and data types of the fields must be specified in the table.</p> <p>Important: All the above points must match the imported table.</p> <p>If the column names have uppercase characters in the imported table when data is output to Cassandra, the job will fail as column names are stored there in lowercase only. You can resolve this issue with the Transformer stage. The output recorded to STDOUT storage can be seen in Logs. The number of records shown in the logs can be specified in the Quantity field. The available range is from 1 to 2147483631 records. For Redis source in Write stage, the following fields must be specified: Key column, Model, SSL, TTL, Table and Write mode.</p> <ul> <li>Key column. For writing. It specifies the unique column used as the Redis key. By default, the key is auto-generated.</li> </ul> <p></p> <ul> <li>TTL. Data expiration time in seconds. Data doesn't expire if TTL is negative or 0. By default, it is 0. A positive value of TTL means the number of seconds in which the data will be removed.</li> </ul> <p>Important: The Write mode field defines how data is posted to its destination. Available values are:</p> <ul> <li>Overwrite</li> <li>Append</li> <li>Error if Exists</li> </ul> <p>With Overwrite Write mode Truncate mode can be used for DB2, Oracle, MySQL, PostgreSQL, MSSQL and Redshift:</p> <ul> <li>None. No truncation occurs, but the target table is deleted and recreated. Note that all the indexes, constraints and other modifiers assigned for this table will be lost.</li> <li>Simple. The standard truncation that deletes data from the target table but keeps the indexes, constraints and other modifiers intact. However, if the target table has a primary key referenced as a foreign key in other ta- bles, the truncation will fail. To resolve this, either use Cascade mode or manually drop constraints (outside of VF) prior to accessing the table with VF.</li> <li>Cascade (only for Oracle and PostgreSQL). The cascade truncation not only deletes the data from the target table but also from other tables that use the target table's primary key as a foreign key constraint.</li> </ul> <p>File format is to choose a format of the destination file. Available formats are:</p> <ul> <li>CSV</li> <li>JSON</li> <li>Parquet</li> <li>ORC</li> <li>Text</li> <li>Avro</li> </ul> <p>Confirm the stage by pushing Confirm on the panel. Now there are two stages to connect.</p> <p></p> <p>Important: to connect stages, hover a mouse on a stage edge until you see a green rectangle. Click and drag it to the border of another stage and its green rectangle. When you reach it, a green arrow should appear.</p> <p> </p> <p>Other stages available are:</p> <ul> <li>Group By</li> <li>Remove Duplicates</li> <li>Filter</li> <li>Transformer</li> <li>Sort</li> <li>Slice</li> <li>Pivot</li> <li>Add/Update Column</li> <li>String Functions</li> <li>Date/Time</li> <li>Drop/Fill Nulls</li> <li>Join</li> <li>Union</li> <li>Change Data Capture</li> <li>Cache</li> <li>Validate</li> </ul>"},{"location":"user-guide/job-operations/create-a-job-available-stages/create-a-job/","title":"Create a Job.","text":"<p>With the Add Job button pushed you get to Job Designer to create a new job.</p> <p></p> <p>You must provide a name for the job on the left configuration panel. Tags can be used to classify your job. Update parameters or keep their default values and then push the Confirm button:</p> <p></p> <p>Save the job by pushing the Save button on Job Designer header. Now you see the Palette tab with all available stages:</p> <p></p>"},{"location":"user-guide/job-operations/job-designer-functions-overview/","title":"Job Designer functions overview","text":"<p>The following functions are available in Job Designer:</p> <ul> <li>Zoom operations:</li> <li>Switch mode between Move  and Select </li> <li>Show job status </li> <li>Run job  / Stop job  (for running)</li> <li>Save job </li> <li>See job logs </li> <li>See job history </li> <li>Undo / Redo operation on canvas  </li> <li>Remove element from canvas </li> <li>Refresh </li> <li>Auto refresh </li> </ul> <p>Note: you can copy, edit or delete a stage on canvas using the mouse right-click menu options:</p> <ul> <li>Edit stage</li> <li>Copy stage</li> <li>Delete stage</li> </ul>"},{"location":"user-guide/job-operations/job-execution/","title":"Job Execution","text":"<p>Push the Play button  to run the job:</p> <p>Its status changes from Draft to Pending </p> <p>Push Refresh to update the status.  It should turn to Running </p> <p>While running, it can be interrupted with the Stop button.  When a job is completed, its status is Succeeded or Failed. Use the Logs button  to analyze job logs. It gets you to Logs screen:</p> <p></p> <p>Logs can be filtered by level. Such as:</p> <ul> <li>INFO</li> <li>WARNING</li> <li>ERROR</li> <li>DEBUG</li> <li>RESULT</li> </ul> <p>You can also view earlier job runs data with the Job History button:</p> <p></p> <p>It takes you to Job History screen, containing each job run data, including logs:</p> <p></p>"},{"location":"user-guide/job-operations/jobs-overview/","title":"Jobs Overview","text":"<p>Clicking the Jobs menu item leads to the Jobs overview screen, which allows you to see a list of jobs existing within a project. If a job is used in a pipeline, it is indicated by  (pipeline) icon. The Jobs overview screen displays the following information:</p> <ul> <li>Job Name</li> <li>Job Last run/Last finished/Last edit</li> <li>Job Status</li> <li>Resource Utilization (CPU/Memory)</li> <li>Available Actions (Run/Job Designer/Logs/Copy/Job History/ Delete)</li> </ul> <p>A job has a certain status at various phases of execution:</p> <ul> <li>Draft</li> <li>Pending</li> <li>Running</li> <li>Succeeded</li> <li>Failed</li> <li>Unknown (This status appears very rarely in case of undefined error)</li> </ul> <p>Notes:</p> <ul> <li>The actions availability and, therefore, visibility depends on user authorizations</li> <li>One cannot delete a job that compounds a pipeline</li> </ul> <p></p>"},{"location":"user-guide/pipeline-operations/create-a-pipeline/","title":"Create a Pipeline","text":"<p>With the Add Pipeline button pushed, you get to Pipeline Designer for creating a pipeline. On the left configuration panel the Params tab is open by default, where you can enter a pipeline name and tags for the pipeline classification:</p> <p></p> <p>Once you add a name, notifications become available to you:</p> <p></p> <p>Set notifications to be notified on the pipeline events as you wish and push the Confirm button.</p> <p>Note: you can set it specifically for Slack or email notifications.</p> <p>Save the pipeline by pressing the Save button on Pipeline Designer header. Once a pipeline is saved the Palette tab with all available stages is opened by default:</p> <p></p> <p>Pipeline is a combination of Job, Pipeline, Notification, Container and Wait stages.</p> <p>The Notification stage is most often added to the configuration to notify about job/pipeline stage failure/success.</p> <p>Drag the Job stage to the canvas:</p> <p></p> <p>Double-click on the stage opens the configuration panel on the right:</p> <p></p> <p>Enter the name for the stage and push  to select a job.</p> <p></p> <p>Pick the job and push the Confirm button. Save the stage by pushing the Confirm button on the panel. Push the Save button on the header if you want to save the pipeline at this step. Similarly to the Job stage the Pipeline stage can be used if you want to evoke existing pipeline within your pipeline. Drag and configure other stages. Connect them in the same manner as in Job Designer.</p> <p>Note: you can also add a stage by double clicking its tile on the palette.</p> <p>You can link stages based on their success or failure. After connecting them to each other you can choose the Success or Failure link on the Job stage configuration panel. There can be only one connection for failure. See the example of configured pipeline:</p> <p></p> <p>Before the first run or after updating, the status of the pipeline is Draft. See each stage border painted in a Gray color, which stands for Draft.</p> <p>The Container stage is used to run custom commands for executing any logic in a pipeline. You can use docker images</p> <p>instead of custom commands. Start creating a pipeline by dragging the Container stage to the canvas and entering parameters in the Configuration panel:</p> <p></p> <p>The Container stage has the following fields in the configuration: * Image link. Docker image path. Examples: * mysql, mysql:latest, * bitnami/argo-cd:2.1.2, localhost:5000/bitnami/argo-cd:2.1.2, registry.redhat.io/rhel7:latest. * Image pull policy. Defines when the image is pulled (downloaded). Possible values:</p> <ul> <li> <p>If not present \u2013 is downloaded only if it does not exist locally;</p> </li> <li> <p>Always \u2013 is downloaded before each start;</p> </li> <li> <p>Never \u2013 is not downloaded, a local copy is used.</p> </li> <li> <p>Requests and Limits CPU</p> </li> <li>Requests and Limits memory</li> <li>Mount project parameters. Defines whether to mount all project parameters as environment variables inside the Pod.</li> <li>Authentication type</li> <li> <p>Authentication mode can be one of these:</p> </li> <li> <p>Not applicable: image pull secrets are not required as the image is pulled from the public registry;</p> </li> <li>New: create a new image pull secret on the fly by providing all necessary information;</li> <li> <p>Provided: use the existing image pull secret by providing its name (Image pull secret name).</p> </li> <li> <p>Image pull secret name. Name of the secret to pull the image. Note that it must exist within the same k8s namespace as the current pipeline.</p> </li> <li>Username</li> <li>Password</li> <li>Registry. Name of the registry for authentication.</li> <li>Command. The command to run once the Pod is created.</li> </ul> <p>Important: the Container stage has Logs button   so you can see logs. If the pipeline completed successfully, the logs display the message contained in the Command field in the configuration of the Container stage.</p> <p></p> <p>The Wait stage is a dummy stage with no configuration, used for running multiple jobs in parallel as in this example:</p> <p></p>"},{"location":"user-guide/pipeline-operations/pipeline-designer-functions-overview/","title":"Pipeline Designer Functions Overview","text":"<p>The following functions are available in Pipeline Designer:</p> <ul> <li>Zoom functions: </li> <li>Move elements: </li> <li>Move elements/screen: </li> <li>Show pipeline status: </li> <li>Show pipeline progress: </li> <li>Run pipeline  / Stop pipeline  (for running)</li> <li>Save pipeline </li> <li>Create schedule for pipeline </li> <li>View pipeline history </li> <li>Undo / Redo operation on canvas </li> <li>Remove element from canvas </li> <li>Refresh </li> <li>Auto refresh </li> </ul>"},{"location":"user-guide/pipeline-operations/pipeline-execution/","title":"Pipeline Execution","text":"<p>If you run a pipeline, as in the above example, its status changes from Draft to Pending and then to Running.  Push Refresh to update the status. The border of the stage currently running is painted in Blue: While running a pipeline can be stopped or suspended with Stop/Suspend buttons respectively: </p> <p></p> <p>Once suspended it can be resumed with Resume button:</p> <p></p> <p>If a pipeline succeeds, all completed stages are painted in Green, indicating success. The stages configured for failure scenario (red arrow) remain Gray as a Draft as they have not been executed.</p> <p></p> <p>If a pipeline fails, then the Red border indicates the failed stage:</p> <p></p> <p>A failed pipeline can be re-run with  button from Pipelines overview page.</p> <p>Important: Job stage has the Logs button  for analyzing logs of a certain job. Use the History button  to view information about previous runs of the pipeline.</p>"},{"location":"user-guide/pipeline-operations/pipelines-overview/","title":"Pipelines Overview","text":"<p>Clicking the Pipelines menu item takes you to the Pipelines overview screen, which shows a list of pipelines existing within a project.</p> <p>It displays the following information:</p> <ul> <li>Pipeline Name</li> <li>Checkbox for deleting/exporting multiple pipelines</li> <li>Pipeline Last run/Last finished/Last edit</li> <li>Pipeline Status</li> <li>Pipeline Progress</li> <li>Available Actions (Run/Pipeline Designer/ Scheduling /History/Copy/Delete)</li> </ul> <p>Pipeline has a certain status at various phases of execution:</p> <ul> <li>Draft</li> <li>Error (This status may appear due to incorrectly entered data)</li> <li>Failed</li> <li>Pending</li> <li>Running</li> <li>Succeeded</li> <li>Suspended</li> <li>Terminated</li> </ul> <p></p> <p>Note: the actions availability and therefore visibility depends on user authorizations.</p>"},{"location":"user-guide/pipeline-operations/scheduling-a-pipeline/","title":"Scheduling a pipeline.","text":"<p>You can schedule a pipeline to run with the Scheduling button:</p> <p></p> <p>It opens the Scheduling window:</p> <p></p> <p>Switch the toggle on and enter values of minute/day/day of month/month/day of the week through spaces according to the tips:</p> <p></p>"},{"location":"user-guide/project-operations/create-a-project/","title":"Create a Project","text":"<p>To create a project, you need to push the \u201c+\u201d button.</p> <p></p> <p>Note: this is the action of the Super-admin user only. The button is not visible for the application roles (Viewer, Operator, Editor, Admin).</p> <p>With the \u201c+\u201d button pushed, you get to Create Project Form to enter basic project settings:</p> <ul> <li>Project Name\uf020</li> <li>Project Description\uf020</li> <li>Requests (CPU/Memory)\uf020</li> <li>Limits (CPU/Memory)</li> </ul> <p></p> <p>After saving Create Project Form the project is created under the given name and then can be found on the initial screen:</p> <p></p>"},{"location":"user-guide/project-operations/getting-started/","title":"Getting started","text":"<p>Once you first log on to the application, you see the initial screen with all existing projects:</p> <p></p> <p>If you are not authorized for a particular project, it is locked for you, so you see the lock icon on its tile. Please contact project owners to get access to their projects. If you click on the user icon in the top right corner, you get to user profile menu: </p> <p>Here you can view your user profile or log out. Also it displays the application current version number.</p>"},{"location":"user-guide/project-operations/manage-project-settings/","title":"Manage Project Settings","text":"<p>The Settings submenu contains:</p> <ul> <li>Basic</li> <li>Parameters</li> <li>Connections</li> <li>Users/Roles</li> </ul> <p>1) The Basic is already there after project creation. The Edit button turns on the edit mode for updates.</p> <p></p> <p>2) The Parameters stores values required for the entire project, e.g., JDBC connection, DB2 credentials, or table schema can be the same for multiple jobs within a project and therefore stored at the project level. The Create Parameter button opens dialog on the right so you can introduce a new parameter.</p> <p></p> <p>3) The Connections option enables the user to manage connections to a storage. Here you see a list of all existing connections with their name/storage type and available actions (view, edit, delete, ping). Also, you can create a new connection with the Create Connection button.</p> <p></p> <p>4) The Users/Roles is meant for user access management or to view user access depending on your authorization. Users cannot set roles to themselves. This operation can be performed by Admin or Super- admin only. So if you try to change your role, you will get the error message: \u201cYou cannot change your role\u201d. The Edit button and therefore Edit mode is only available for an Admin within the project or for a Super- admin.</p> <p></p>"},{"location":"user-guide/project-operations/project-overview/","title":"Project Overview","text":"<p>The screen contains the project left menu and displays information about the project jobs, pipelines and resource utilization (applicable for running jobs).</p> <p> </p>"},{"location":"user-guide/roles-and-authorizations/","title":"Roles and authorizations","text":"<p>The following roles are available in the application:</p> <ul> <li>Viewer</li> <li>Operator</li> <li>Editor</li> <li>Administrator</li> </ul> <p>They enable one to perform the below operations within the namespaces they contain access to. Only a Super-admin user can create and delete a workspace (project) and grant initial access to this project.</p>"},{"location":"user-guide/roles-and-authorizations/#actions-table","title":"Actions Table","text":"Role Project Settings Jobs Pipelines Viewer View All View All View All Operator View All View All / execute jobs View All / execute pipelines Editor Edit All but Users/Roles Edit / execute jobs Edit / execute pipelines Admin Edit All Edit / execute jobs Edit / execute pipelines"}]}